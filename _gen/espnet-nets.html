

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>espnet.nets package &mdash; ESPnet 0.4.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="espnet.transform package" href="espnet-transform.html" />
    <link rel="prev" title="espnet.lm package" href="espnet-lm.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> ESPnet
          

          
          </a>

          
            
            
              <div class="version">
                0.4.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Tutorial:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Outline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html#execution-of-example-scripts">Execution of example scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html#demonstration-using-pretrained-models">Demonstration using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html#installation-using-docker">Installation using Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html#references">References</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="espnet-asr.html">espnet.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet-lm.html">espnet.lm package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">espnet.nets package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-asr-interface">espnet.nets.asr_interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend">espnet.nets.chainer_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-attentions">espnet.nets.chainer_backend.attentions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-attentions-transformer">espnet.nets.chainer_backend.attentions_transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-ctc">espnet.nets.chainer_backend.ctc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-decoders">espnet.nets.chainer_backend.decoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-decoders-transformer">espnet.nets.chainer_backend.decoders_transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-deterministic-embed-id">espnet.nets.chainer_backend.deterministic_embed_id</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-e2e-asr">espnet.nets.chainer_backend.e2e_asr</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-e2e-asr-transformer">espnet.nets.chainer_backend.e2e_asr_transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-encoders">espnet.nets.chainer_backend.encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-encoders-transformer">espnet.nets.chainer_backend.encoders_transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-nets-utils">espnet.nets.chainer_backend.nets_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-chainer-backend-nets-utils-transformer">espnet.nets.chainer_backend.nets_utils_transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-ctc-prefix-score">espnet.nets.ctc_prefix_score</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-e2e-asr-common">espnet.nets.e2e_asr_common</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend">espnet.nets.pytorch_backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-ctc">espnet.nets.pytorch_backend.ctc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-e2e-asr">espnet.nets.pytorch_backend.e2e_asr</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-e2e-asr-mix">espnet.nets.pytorch_backend.e2e_asr_mix</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-e2e-asr-transformer">espnet.nets.pytorch_backend.e2e_asr_transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-e2e-tts-fastspeech">espnet.nets.pytorch_backend.e2e_tts_fastspeech</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-e2e-tts-tacotron2">espnet.nets.pytorch_backend.e2e_tts_tacotron2</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-e2e-tts-transformer">espnet.nets.pytorch_backend.e2e_tts_transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-fastspeech">espnet.nets.pytorch_backend.fastspeech</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-frontends">espnet.nets.pytorch_backend.frontends</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-nets-utils">espnet.nets.pytorch_backend.nets_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-rnn">espnet.nets.pytorch_backend.rnn</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-streaming">espnet.nets.pytorch_backend.streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-tacotron2">espnet.nets.pytorch_backend.tacotron2</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-pytorch-backend-transformer">espnet.nets.pytorch_backend.transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#espnet-nets-tts-interface">espnet.nets.tts_interface</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="espnet-transform.html">espnet.transform package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet-tts.html">espnet.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet-utils.html">espnet.utils package</a></li>
</ul>
<p class="caption"><span class="caption-text">Tool Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../apis/espnet_bin.html">core tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_py.html">python utility tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_sh.html">bash utility tools</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ESPnet</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>espnet.nets package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/_gen/espnet-nets.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="espnet-nets-package">
<h1>espnet.nets package<a class="headerlink" href="#espnet-nets-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="espnet-nets-asr-interface">
<span id="id1"></span><h2>espnet.nets.asr_interface<a class="headerlink" href="#espnet-nets-asr-interface" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.asr_interface"></span><dl class="class">
<dt id="espnet.nets.asr_interface.ASRInterface">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.asr_interface.</code><code class="sig-name descname">ASRInterface</code><a class="reference internal" href="../_modules/espnet/nets/asr_interface.html#ASRInterface"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.asr_interface.ASRInterface" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>ASR Interface for ESPnet model implementation</p>
<dl class="method">
<dt id="espnet.nets.asr_interface.ASRInterface.add_arguments">
<em class="property">static </em><code class="sig-name descname">add_arguments</code><span class="sig-paren">(</span><em class="sig-param">parser</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/asr_interface.html#ASRInterface.add_arguments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.asr_interface.ASRInterface.add_arguments" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.asr_interface.ASRInterface.attention_plot_class">
<em class="property">property </em><code class="sig-name descname">attention_plot_class</code><a class="headerlink" href="#espnet.nets.asr_interface.ASRInterface.attention_plot_class" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.asr_interface.ASRInterface.calculate_all_attentions">
<code class="sig-name descname">calculate_all_attentions</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/asr_interface.html#ASRInterface.calculate_all_attentions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.asr_interface.ASRInterface.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>attention calculation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>list</em>) – list of padded input sequences [(T1, idim), (T2, idim), …]</p></li>
<li><p><strong>ilens</strong> (<em>ndarray</em>) – batch of lengths of input sequences (B)</p></li>
<li><p><strong>ys</strong> (<em>list</em>) – list of character id sequence tensor [(L1), (L2), (L3), …]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>attention weights (B, Lmax, Tmax)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.asr_interface.ASRInterface.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/asr_interface.html#ASRInterface.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.asr_interface.ASRInterface.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>compute loss for training</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> – For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim)
For chainer, list of source sequences chainer.Variable</p></li>
<li><p><strong>ilens</strong> – batch of lengths of source sequences (B)
For pytorch, torch.Tensor
For chainer, list of int</p></li>
<li><p><strong>ys</strong> – For pytorch, batch of padded source sequences torch.Tensor (B, Lmax)
For chainer, list of source sequences chainer.Variable</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor for pytorch, chainer.Variable for chainer</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.asr_interface.ASRInterface.recognize">
<code class="sig-name descname">recognize</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">recog_args</em>, <em class="sig-param">char_list=None</em>, <em class="sig-param">rnnlm=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/asr_interface.html#ASRInterface.recognize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.asr_interface.ASRInterface.recognize" title="Permalink to this definition">¶</a></dt>
<dd><p>recognize x for evaluation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>ndarray</em>) – input acouctic feature (B, T, D) or (T, D)</p></li>
<li><p><strong>recog_args</strong> (<em>namespace</em>) – argment namespace contraining options</p></li>
<li><p><strong>char_list</strong> (<em>list</em>) – list of characters</p></li>
<li><p><strong>rnnlm</strong> (<em>torch.nn.Module</em>) – language model module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best decoding results</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend">
<span id="id2"></span><h2>espnet.nets.chainer_backend<a class="headerlink" href="#espnet-nets-chainer-backend" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend"></span></div>
<div class="section" id="espnet-nets-chainer-backend-attentions">
<span id="id3"></span><h2>espnet.nets.chainer_backend.attentions<a class="headerlink" href="#espnet-nets-chainer-backend-attentions" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.attentions"></span><dl class="class">
<dt id="espnet.nets.chainer_backend.attentions.AttDot">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.attentions.</code><code class="sig-name descname">AttDot</code><span class="sig-paren">(</span><em class="sig-param">eprojs</em>, <em class="sig-param">dunits</em>, <em class="sig-param">att_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/attentions.html#AttDot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.attentions.AttDot" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="espnet.nets.chainer_backend.attentions.AttDot.reset">
<code class="sig-name descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/attentions.html#AttDot.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.attentions.AttDot.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.attentions.AttLoc">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.attentions.</code><code class="sig-name descname">AttLoc</code><span class="sig-paren">(</span><em class="sig-param">eprojs</em>, <em class="sig-param">dunits</em>, <em class="sig-param">att_dim</em>, <em class="sig-param">aconv_chans</em>, <em class="sig-param">aconv_filts</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/attentions.html#AttLoc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.attentions.AttLoc" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="espnet.nets.chainer_backend.attentions.AttLoc.reset">
<code class="sig-name descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/attentions.html#AttLoc.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.attentions.AttLoc.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.attentions.NoAtt">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.attentions.</code><code class="sig-name descname">NoAtt</code><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/attentions.html#NoAtt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.attentions.NoAtt" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="espnet.nets.chainer_backend.attentions.NoAtt.reset">
<code class="sig-name descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/attentions.html#NoAtt.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.attentions.NoAtt.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>reset states</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet.nets.chainer_backend.attentions.att_for">
<code class="sig-prename descclassname">espnet.nets.chainer_backend.attentions.</code><code class="sig-name descname">att_for</code><span class="sig-paren">(</span><em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/attentions.html#att_for"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.attentions.att_for" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an attention given the program arguments</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> (<em>Namespace</em>) – the arguments</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The corresponding attention module</p>
</dd>
</dl>
<p>:rtype chainer.Chain</p>
</dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend-attentions-transformer">
<span id="id4"></span><h2>espnet.nets.chainer_backend.attentions_transformer<a class="headerlink" href="#espnet-nets-chainer-backend-attentions-transformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.attentions_transformer"></span><dl class="class">
<dt id="espnet.nets.chainer_backend.attentions_transformer.MultiHeadAttention">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.attentions_transformer.</code><code class="sig-name descname">MultiHeadAttention</code><span class="sig-paren">(</span><em class="sig-param">n_units</em>, <em class="sig-param">h=8</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">initialW=None</em>, <em class="sig-param">initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/attentions_transformer.html#MultiHeadAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.attentions_transformer.MultiHeadAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<p>Multi Head Attention Layer</p>
</dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend-ctc">
<span id="id5"></span><h2>espnet.nets.chainer_backend.ctc<a class="headerlink" href="#espnet-nets-chainer-backend-ctc" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.ctc"></span><dl class="class">
<dt id="espnet.nets.chainer_backend.ctc.CTC">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.ctc.</code><code class="sig-name descname">CTC</code><span class="sig-paren">(</span><em class="sig-param">odim</em>, <em class="sig-param">eprojs</em>, <em class="sig-param">dropout_rate</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/ctc.html#CTC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.ctc.CTC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="espnet.nets.chainer_backend.ctc.CTC.log_softmax">
<code class="sig-name descname">log_softmax</code><span class="sig-paren">(</span><em class="sig-param">hs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/ctc.html#CTC.log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.ctc.CTC.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>log_softmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs</strong> – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.ctc.WarpCTC">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.ctc.</code><code class="sig-name descname">WarpCTC</code><span class="sig-paren">(</span><em class="sig-param">odim</em>, <em class="sig-param">eprojs</em>, <em class="sig-param">dropout_rate</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/ctc.html#WarpCTC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.ctc.WarpCTC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="espnet.nets.chainer_backend.ctc.WarpCTC.log_softmax">
<code class="sig-name descname">log_softmax</code><span class="sig-paren">(</span><em class="sig-param">hs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/ctc.html#WarpCTC.log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.ctc.WarpCTC.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>log_softmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs</strong> – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet.nets.chainer_backend.ctc.ctc_for">
<code class="sig-prename descclassname">espnet.nets.chainer_backend.ctc.</code><code class="sig-name descname">ctc_for</code><span class="sig-paren">(</span><em class="sig-param">args</em>, <em class="sig-param">odim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/ctc.html#ctc_for"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.ctc.ctc_for" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the CTC corresponding to the args</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> (<em>Namespace</em>) – The program arguments</p></li>
<li><p><strong>odim</strong> (<em>int</em>) – The output dimension</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The CTC module</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend-decoders">
<span id="id6"></span><h2>espnet.nets.chainer_backend.decoders<a class="headerlink" href="#espnet-nets-chainer-backend-decoders" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.decoders"></span><dl class="class">
<dt id="espnet.nets.chainer_backend.decoders.Decoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.decoders.</code><code class="sig-name descname">Decoder</code><span class="sig-paren">(</span><em class="sig-param">eprojs</em>, <em class="sig-param">odim</em>, <em class="sig-param">dtype</em>, <em class="sig-param">dlayers</em>, <em class="sig-param">dunits</em>, <em class="sig-param">sos</em>, <em class="sig-param">eos</em>, <em class="sig-param">att</em>, <em class="sig-param">verbose=0</em>, <em class="sig-param">char_list=None</em>, <em class="sig-param">labeldist=None</em>, <em class="sig-param">lsm_weight=0.0</em>, <em class="sig-param">sampling_probability=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/decoders.html#Decoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.decoders.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="espnet.nets.chainer_backend.decoders.Decoder.calculate_all_attentions">
<code class="sig-name descname">calculate_all_attentions</code><span class="sig-paren">(</span><em class="sig-param">hs</em>, <em class="sig-param">ys</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/decoders.html#Decoder.calculate_all_attentions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.decoders.Decoder.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate all of attentions</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of attentions</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.decoders.Decoder.recognize_beam">
<code class="sig-name descname">recognize_beam</code><span class="sig-paren">(</span><em class="sig-param">h</em>, <em class="sig-param">lpz</em>, <em class="sig-param">recog_args</em>, <em class="sig-param">char_list</em>, <em class="sig-param">rnnlm=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/decoders.html#Decoder.recognize_beam"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.decoders.Decoder.recognize_beam" title="Permalink to this definition">¶</a></dt>
<dd><p>beam search implementation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>h</strong> – </p></li>
<li><p><strong>lpz</strong> – </p></li>
<li><p><strong>recog_args</strong> – </p></li>
<li><p><strong>char_list</strong> – </p></li>
<li><p><strong>rnnlm</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.decoders.Decoder.rnn_forward">
<code class="sig-name descname">rnn_forward</code><span class="sig-paren">(</span><em class="sig-param">ey</em>, <em class="sig-param">z_list</em>, <em class="sig-param">c_list</em>, <em class="sig-param">z_prev</em>, <em class="sig-param">c_prev</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/decoders.html#Decoder.rnn_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.decoders.Decoder.rnn_forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet.nets.chainer_backend.decoders.decoder_for">
<code class="sig-prename descclassname">espnet.nets.chainer_backend.decoders.</code><code class="sig-name descname">decoder_for</code><span class="sig-paren">(</span><em class="sig-param">args</em>, <em class="sig-param">odim</em>, <em class="sig-param">sos</em>, <em class="sig-param">eos</em>, <em class="sig-param">att</em>, <em class="sig-param">labeldist</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/decoders.html#decoder_for"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.decoders.decoder_for" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend-decoders-transformer">
<span id="id7"></span><h2>espnet.nets.chainer_backend.decoders_transformer<a class="headerlink" href="#espnet-nets-chainer-backend-decoders-transformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.decoders_transformer"></span><dl class="class">
<dt id="espnet.nets.chainer_backend.decoders_transformer.Decoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.decoders_transformer.</code><code class="sig-name descname">Decoder</code><span class="sig-paren">(</span><em class="sig-param">odim</em>, <em class="sig-param">n_layers</em>, <em class="sig-param">n_units</em>, <em class="sig-param">d_units=0</em>, <em class="sig-param">h=8</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">initialW=None</em>, <em class="sig-param">initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/decoders_transformer.html#Decoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.decoders_transformer.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.decoders_transformer.DecoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.decoders_transformer.</code><code class="sig-name descname">DecoderLayer</code><span class="sig-paren">(</span><em class="sig-param">n_units</em>, <em class="sig-param">d_units=0</em>, <em class="sig-param">h=8</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">initialW=None</em>, <em class="sig-param">initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/decoders_transformer.html#DecoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.decoders_transformer.DecoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend-deterministic-embed-id">
<span id="id8"></span><h2>espnet.nets.chainer_backend.deterministic_embed_id<a class="headerlink" href="#espnet-nets-chainer-backend-deterministic-embed-id" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.deterministic_embed_id"></span><dl class="class">
<dt id="espnet.nets.chainer_backend.deterministic_embed_id.EmbedID">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.deterministic_embed_id.</code><code class="sig-name descname">EmbedID</code><span class="sig-paren">(</span><em class="sig-param">in_size</em>, <em class="sig-param">out_size</em>, <em class="sig-param">initialW=None</em>, <em class="sig-param">ignore_label=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/deterministic_embed_id.html#EmbedID"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedID" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Link</span></code></p>
<p>Efficient linear layer for one-hot input.</p>
<p>This is a link that wraps the <code class="xref py py-func docutils literal notranslate"><span class="pre">embed_id()</span></code> function.
This link holds the ID (word) embedding matrix <code class="docutils literal notranslate"><span class="pre">W</span></code> as a parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_size</strong> (<em>int</em>) – Number of different identifiers (a.k.a. vocabulary size)</p></li>
<li><p><strong>out_size</strong> (<em>int</em>) – Initializer to initialize the weight. When it is np.ndarray, its ndim should be 2</p></li>
<li><p><strong>ignore_label</strong> (<em>int</em>) – If <cite>ignore_label</cite> is an int value, i-th column of return value is filled with 0</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">embed_id()</span></code></p>
</div>
<dl class="attribute">
<dt id="espnet.nets.chainer_backend.deterministic_embed_id.EmbedID.W">
<code class="sig-name descname">W</code><a class="headerlink" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedID.W" title="Permalink to this definition">¶</a></dt>
<dd><p>Embedding parameter matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Variable</p>
</dd>
</dl>
</dd></dl>

<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W</span>
<span class="go">array([[ 0.,  0.,  0.],</span>
<span class="go">       [ 1.,  1.,  1.],</span>
<span class="go">       [ 2.,  2.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">EmbedID</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">initialW</span><span class="o">=</span><span class="n">W</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;i&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">array([2, 1], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">data</span>
<span class="go">array([[ 2.,  2.,  2.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
</pre></div>
</div>
</div>
<dl class="attribute">
<dt id="espnet.nets.chainer_backend.deterministic_embed_id.EmbedID.ignore_label">
<code class="sig-name descname">ignore_label</code><em class="property"> = None</em><a class="headerlink" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedID.ignore_label" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.deterministic_embed_id.</code><code class="sig-name descname">EmbedIDFunction</code><span class="sig-paren">(</span><em class="sig-param">ignore_label=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/deterministic_embed_id.html#EmbedIDFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.function_node.FunctionNode</span></code></p>
<dl class="method">
<dt id="espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">indexes</em>, <em class="sig-param">grad_outputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/deterministic_embed_id.html#EmbedIDFunction.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes gradients w.r.t. specified inputs given output gradients.</p>
<p>This method is used to compute one step of the backpropagation
corresponding to the forward computation of this function node.
Given the gradients w.r.t. output variables, this method computes the
gradients w.r.t. specified input variables. Note that this method does
not need to compute any input gradients not specified by
<code class="docutils literal notranslate"><span class="pre">target_input_indices</span></code>.</p>
<p>Unlike <code class="xref py py-meth docutils literal notranslate"><span class="pre">Function.backward()</span></code>,
gradients are given as  <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> objects and this
method itself has to return input gradients as
<code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> objects. It enables the function node to
return the input gradients with the full computational history, in
which case it supports <em>differentiable backpropagation</em> or
<em>higher-order differentiation</em>.</p>
<p>The default implementation returns <code class="docutils literal notranslate"><span class="pre">None</span></code> s, which means the
function is not differentiable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target_input_indexes</strong> (<em>tuple of int</em>) – Sorted indices of the input
variables w.r.t. which the gradients are required. It is
guaranteed that this tuple contains at least one element.</p></li>
<li><p><strong>grad_outputs</strong> (tuple of <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code>s) – Gradients
w.r.t. the output variables.
If the gradient w.r.t. an output variable is not
given, the corresponding element is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tuple of variables that represent the gradients w.r.t. specified
input variables. The length of the tuple can be same as either
<code class="docutils literal notranslate"><span class="pre">len(target_input_indexes)</span></code> or the number of inputs. In the
latter case, the elements not specified by <code class="docutils literal notranslate"><span class="pre">target_input_indexes</span></code>
will be discarded.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward_accumulate()</span></code> provides an alternative interface that
allows you to implement the backward computation fused with the
gradient accumulation.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction.check_type_forward">
<code class="sig-name descname">check_type_forward</code><span class="sig-paren">(</span><em class="sig-param">in_types</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/deterministic_embed_id.html#EmbedIDFunction.check_type_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction.check_type_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks types of input data before forward propagation.</p>
<p>This method is called before <a class="reference internal" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction.forward" title="espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> and validates the types of
input variables using
<span class="xref std std-ref">the type checking utilities</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>in_types</strong> (<em>TypeInfoTuple</em>) – The type
information of input variables for <a class="reference internal" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction.forward" title="espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/deterministic_embed_id.html#EmbedIDFunction.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDFunction.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output arrays from the input arrays.</p>
<p>It delegates the procedure to <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward_cpu()</span></code> or
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward_gpu()</span></code> by default. Which of them this method selects is
determined by the type of input arrays. Implementations of
<code class="xref py py-class docutils literal notranslate"><span class="pre">FunctionNode</span></code> must implement either CPU/GPU methods or this
method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – Tuple of input array(s).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tuple of output array(s).</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Implementations of <code class="xref py py-class docutils literal notranslate"><span class="pre">FunctionNode</span></code> must take care that the
return value must be a tuple even if it returns only one array.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDGrad">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.deterministic_embed_id.</code><code class="sig-name descname">EmbedIDGrad</code><span class="sig-paren">(</span><em class="sig-param">w_shape</em>, <em class="sig-param">ignore_label=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/deterministic_embed_id.html#EmbedIDGrad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDGrad" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.function_node.FunctionNode</span></code></p>
<dl class="method">
<dt id="espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDGrad.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">indexes</em>, <em class="sig-param">grads</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/deterministic_embed_id.html#EmbedIDGrad.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDGrad.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes gradients w.r.t. specified inputs given output gradients.</p>
<p>This method is used to compute one step of the backpropagation
corresponding to the forward computation of this function node.
Given the gradients w.r.t. output variables, this method computes the
gradients w.r.t. specified input variables. Note that this method does
not need to compute any input gradients not specified by
<code class="docutils literal notranslate"><span class="pre">target_input_indices</span></code>.</p>
<p>Unlike <code class="xref py py-meth docutils literal notranslate"><span class="pre">Function.backward()</span></code>,
gradients are given as  <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> objects and this
method itself has to return input gradients as
<code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> objects. It enables the function node to
return the input gradients with the full computational history, in
which case it supports <em>differentiable backpropagation</em> or
<em>higher-order differentiation</em>.</p>
<p>The default implementation returns <code class="docutils literal notranslate"><span class="pre">None</span></code> s, which means the
function is not differentiable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target_input_indexes</strong> (<em>tuple of int</em>) – Sorted indices of the input
variables w.r.t. which the gradients are required. It is
guaranteed that this tuple contains at least one element.</p></li>
<li><p><strong>grad_outputs</strong> (tuple of <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code>s) – Gradients
w.r.t. the output variables.
If the gradient w.r.t. an output variable is not
given, the corresponding element is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tuple of variables that represent the gradients w.r.t. specified
input variables. The length of the tuple can be same as either
<code class="docutils literal notranslate"><span class="pre">len(target_input_indexes)</span></code> or the number of inputs. In the
latter case, the elements not specified by <code class="docutils literal notranslate"><span class="pre">target_input_indexes</span></code>
will be discarded.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward_accumulate()</span></code> provides an alternative interface that
allows you to implement the backward computation fused with the
gradient accumulation.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDGrad.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/deterministic_embed_id.html#EmbedIDGrad.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.deterministic_embed_id.EmbedIDGrad.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output arrays from the input arrays.</p>
<p>It delegates the procedure to <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward_cpu()</span></code> or
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward_gpu()</span></code> by default. Which of them this method selects is
determined by the type of input arrays. Implementations of
<code class="xref py py-class docutils literal notranslate"><span class="pre">FunctionNode</span></code> must implement either CPU/GPU methods or this
method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – Tuple of input array(s).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tuple of output array(s).</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Implementations of <code class="xref py py-class docutils literal notranslate"><span class="pre">FunctionNode</span></code> must take care that the
return value must be a tuple even if it returns only one array.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet.nets.chainer_backend.deterministic_embed_id.embed_id">
<code class="sig-prename descclassname">espnet.nets.chainer_backend.deterministic_embed_id.</code><code class="sig-name descname">embed_id</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">W</em>, <em class="sig-param">ignore_label=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/deterministic_embed_id.html#embed_id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.deterministic_embed_id.embed_id" title="Permalink to this definition">¶</a></dt>
<dd><p>Efficient linear function for one-hot input.</p>
<p>This function implements so called <em>word embeddings</em>. It takes two
arguments: a set of IDs (words) <code class="docutils literal notranslate"><span class="pre">x</span></code> in <span class="math notranslate nohighlight">\(B\)</span> dimensional integer
vector, and a set of all ID (word) embeddings <code class="docutils literal notranslate"><span class="pre">W</span></code> in <span class="math notranslate nohighlight">\(V \times d\)</span>
float32 matrix. It outputs <span class="math notranslate nohighlight">\(B \times d\)</span> matrix whose <code class="docutils literal notranslate"><span class="pre">i</span></code>-th
column is the <code class="docutils literal notranslate"><span class="pre">x[i]</span></code>-th column of <code class="docutils literal notranslate"><span class="pre">W</span></code>.</p>
<p>This function is only differentiable on the input <code class="docutils literal notranslate"><span class="pre">W</span></code>.</p>
<p>:param chainer.Variable | np.ndarray x : Batch vectors of IDs. Each element must be signed integer
:param chainer.Variable | np.ndarray W : Distributed representation of each ID (a.k.a. word embeddings)
:param int ignore_label : If ignore_label is an int value, i-th column of return value is filled with 0
:return Output variable
:rtype chainer.Variable</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">EmbedID</span></code></p>
</div>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;i&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">array([2, 1], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W</span>
<span class="go">array([[ 0.,  0.,  0.],</span>
<span class="go">       [ 1.,  1.,  1.],</span>
<span class="go">       [ 2.,  2.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embed_id</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
<span class="go">array([[ 2.,  2.,  2.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embed_id</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">ignore_label</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
<span class="go">array([[ 2.,  2.,  2.],</span>
<span class="go">       [ 0.,  0.,  0.]], dtype=float32)</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend-e2e-asr">
<span id="id9"></span><h2>espnet.nets.chainer_backend.e2e_asr<a class="headerlink" href="#espnet-nets-chainer-backend-e2e-asr" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.e2e_asr"></span><dl class="class">
<dt id="espnet.nets.chainer_backend.e2e_asr.E2E">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.e2e_asr.</code><code class="sig-name descname">E2E</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">odim</em>, <em class="sig-param">args</em>, <em class="sig-param">flag_return=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr.html#E2E"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr.E2E" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet.nets.asr_interface.ASRInterface" title="espnet.nets.asr_interface.ASRInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.asr_interface.ASRInterface</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr.E2E.calculate_all_attentions">
<code class="sig-name descname">calculate_all_attentions</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr.html#E2E.calculate_all_attentions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr.E2E.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E attention calculation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>list</em>) – </p></li>
<li><p><strong>xs</strong> – list of padded input sequences [(T1, idim), (T2, idim), …]</p></li>
<li><p><strong>ilens</strong> (<em>np.ndarray</em>) – batch of lengths of input sequences (B)</p></li>
<li><p><strong>ys</strong> (<em>list</em>) – list of character id sequence tensor [(L1), (L2), (L3), …]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>attention weights (B, Lmax, Tmax)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr.E2E.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr.html#E2E.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr.E2E.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E forward</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> – </p></li>
<li><p><strong>ilens</strong> – </p></li>
<li><p><strong>ys</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr.E2E.recognize">
<code class="sig-name descname">recognize</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">recog_args</em>, <em class="sig-param">char_list</em>, <em class="sig-param">rnnlm=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr.html#E2E.recognize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr.E2E.recognize" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E greedy/beam search</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – </p></li>
<li><p><strong>recog_args</strong> – </p></li>
<li><p><strong>char_list</strong> – </p></li>
<li><p><strong>rnnlm</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend-e2e-asr-transformer">
<span id="id10"></span><h2>espnet.nets.chainer_backend.e2e_asr_transformer<a class="headerlink" href="#espnet-nets-chainer-backend-e2e-asr-transformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.e2e_asr_transformer"></span><dl class="class">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.E2E">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.e2e_asr_transformer.</code><code class="sig-name descname">E2E</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">odim</em>, <em class="sig-param">args</em>, <em class="sig-param">ignore_id=-1</em>, <em class="sig-param">flag_return=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#E2E"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.E2E" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet.nets.asr_interface.ASRInterface" title="espnet.nets.asr_interface.ASRInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.asr_interface.ASRInterface</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.E2E.add_arguments">
<em class="property">static </em><code class="sig-name descname">add_arguments</code><span class="sig-paren">(</span><em class="sig-param">parser</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#E2E.add_arguments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.E2E.add_arguments" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.E2E.attention_plot_class">
<em class="property">property </em><code class="sig-name descname">attention_plot_class</code><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.E2E.attention_plot_class" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.E2E.calculate_all_attentions">
<code class="sig-name descname">calculate_all_attentions</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#E2E.calculate_all_attentions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.E2E.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E attention calculation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>list</em>) – list of padded input sequences [(T1, idim), (T2, idim), …]</p></li>
<li><p><strong>ilens</strong> (<em>ndarray</em>) – batch of lengths of input sequences (B)</p></li>
<li><p><strong>ys</strong> (<em>list</em>) – list of character id sequence tensor [(L1), (L2), (L3), …]</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>attention weights (B, Lmax, Tmax)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.E2E.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em>, <em class="sig-param">calculate_attentions=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#E2E.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.E2E.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>compute loss for training</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> – For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim)
For chainer, list of source sequences chainer.Variable</p></li>
<li><p><strong>ilens</strong> – batch of lengths of source sequences (B)
For pytorch, torch.Tensor
For chainer, list of int</p></li>
<li><p><strong>ys</strong> – For pytorch, batch of padded source sequences torch.Tensor (B, Lmax)
For chainer, list of source sequences chainer.Variable</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor for pytorch, chainer.Variable for chainer</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.E2E.make_attention_mask">
<code class="sig-name descname">make_attention_mask</code><span class="sig-paren">(</span><em class="sig-param">source_block</em>, <em class="sig-param">target_block</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#E2E.make_attention_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.E2E.make_attention_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.E2E.make_history_mask">
<code class="sig-name descname">make_history_mask</code><span class="sig-paren">(</span><em class="sig-param">block</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#E2E.make_history_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.E2E.make_history_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.E2E.output_and_loss">
<code class="sig-name descname">output_and_loss</code><span class="sig-paren">(</span><em class="sig-param">concat_logit_block</em>, <em class="sig-param">t_block</em>, <em class="sig-param">batch</em>, <em class="sig-param">length</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#E2E.output_and_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.E2E.output_and_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.E2E.recognize">
<code class="sig-name descname">recognize</code><span class="sig-paren">(</span><em class="sig-param">x_block</em>, <em class="sig-param">recog_args</em>, <em class="sig-param">char_list=None</em>, <em class="sig-param">rnnlm=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#E2E.recognize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.E2E.recognize" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E beam search</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>ndarray</em>) – input acouctic feature (B, T, D) or (T, D)</p></li>
<li><p><strong>recog_args</strong> (<em>namespace</em>) – argment namespace contraining options</p></li>
<li><p><strong>char_list</strong> (<em>list</em>) – list of characters</p></li>
<li><p><strong>rnnlm</strong> (<em>torch.nn.Module</em>) – language model module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best decoding results</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.E2E.reset_parameters">
<code class="sig-name descname">reset_parameters</code><span class="sig-paren">(</span><em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#E2E.reset_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.E2E.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.PlotAttentionReport">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.e2e_asr_transformer.</code><code class="sig-name descname">PlotAttentionReport</code><span class="sig-paren">(</span><em class="sig-param">att_vis_fn</em>, <em class="sig-param">data</em>, <em class="sig-param">outdir</em>, <em class="sig-param">converter</em>, <em class="sig-param">transform</em>, <em class="sig-param">device</em>, <em class="sig-param">reverse=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#PlotAttentionReport"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.PlotAttentionReport" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="espnet-asr.html#espnet.asr.asr_utils.PlotAttentionReport" title="espnet.asr.asr_utils.PlotAttentionReport"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.asr.asr_utils.PlotAttentionReport</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.VaswaniRule">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.e2e_asr_transformer.</code><code class="sig-name descname">VaswaniRule</code><span class="sig-paren">(</span><em class="sig-param">attr</em>, <em class="sig-param">d</em>, <em class="sig-param">warmup_steps=4000</em>, <em class="sig-param">init=None</em>, <em class="sig-param">target=None</em>, <em class="sig-param">optimizer=None</em>, <em class="sig-param">scale=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#VaswaniRule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.VaswaniRule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.training.extension.Extension</span></code></p>
<p>Trainer extension to shift an optimizer attribute magically by Vaswani.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>attr</strong> (<em>str</em>) – Name of the attribute to shift.</p></li>
<li><p><strong>rate</strong> (<em>float</em>) – Rate of the exponential shift. This value is multiplied
to the attribute at each call.</p></li>
<li><p><strong>init</strong> (<em>float</em>) – Initial value of the attribute. If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, the
extension extracts the attribute at the first call and uses it as
the initial value.</p></li>
<li><p><strong>target</strong> (<em>float</em>) – Target value of the attribute. If the attribute reaches
this value, the shift stops.</p></li>
<li><p><strong>optimizer</strong> (<em>Optimizer</em>) – Target optimizer to adjust the
attribute. If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, the main optimizer of the updater is
used.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.VaswaniRule.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">trainer</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#VaswaniRule.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.VaswaniRule.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Initializes up the trainer state.</p>
<p>This method is called before entering the training loop. An extension
that modifies the state of <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> can
override this method to initialize it.</p>
<p>When the trainer has been restored from a snapshot, this method has to
recover an appropriate part of the state of the trainer.</p>
<p>For example, <code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialShift</span></code>
extension changes the optimizer’s hyperparameter at each invocation.
Note that the hyperparameter is not saved to the snapshot; it is the
responsibility of the extension to recover the hyperparameter.
The <code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialShift</span></code> extension
recovers it in its <code class="docutils literal notranslate"><span class="pre">initialize</span></code> method if it has been loaded from a
snapshot, or just setting the initial value otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>trainer</strong> (<em>Trainer</em>) – Trainer object that runs the training loop.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.chainer_backend.e2e_asr_transformer.VaswaniRule.serialize">
<code class="sig-name descname">serialize</code><span class="sig-paren">(</span><em class="sig-param">serializer</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/e2e_asr_transformer.html#VaswaniRule.serialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.e2e_asr_transformer.VaswaniRule.serialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Serializes the extension state.</p>
<p>It is called when a trainer that owns this extension is serialized. It
serializes nothing by default.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend-encoders">
<span id="id11"></span><h2>espnet.nets.chainer_backend.encoders<a class="headerlink" href="#espnet-nets-chainer-backend-encoders" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.encoders"></span><dl class="class">
<dt id="espnet.nets.chainer_backend.encoders.Encoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.encoders.</code><code class="sig-name descname">Encoder</code><span class="sig-paren">(</span><em class="sig-param">etype</em>, <em class="sig-param">idim</em>, <em class="sig-param">elayers</em>, <em class="sig-param">eunits</em>, <em class="sig-param">eprojs</em>, <em class="sig-param">subsample</em>, <em class="sig-param">dropout</em>, <em class="sig-param">in_channel=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/encoders.html#Encoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.encoders.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<p>Encoder network class</p>
<p>This is the example of docstring.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>etype</strong> (<em>str</em>) – type of encoder network</p></li>
<li><p><strong>idim</strong> (<em>int</em>) – number of dimensions of encoder network</p></li>
<li><p><strong>elayers</strong> (<em>int</em>) – number of layers of encoder network</p></li>
<li><p><strong>eunits</strong> (<em>int</em>) – number of lstm units of encoder network</p></li>
<li><p><strong>eprojs</strong> (<em>int</em>) – number of projection units of encoder network</p></li>
<li><p><strong>subsample</strong> (<em>np.ndarray</em>) – subsampling number e.g. 1_2_2_2_1</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout rate</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.encoders.RNN">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.encoders.</code><code class="sig-name descname">RNN</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">elayers</em>, <em class="sig-param">cdim</em>, <em class="sig-param">hdim</em>, <em class="sig-param">dropout</em>, <em class="sig-param">typ='lstm'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/encoders.html#RNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.encoders.RNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.encoders.RNNP">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.encoders.</code><code class="sig-name descname">RNNP</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">elayers</em>, <em class="sig-param">cdim</em>, <em class="sig-param">hdim</em>, <em class="sig-param">subsample</em>, <em class="sig-param">dropout</em>, <em class="sig-param">typ='blstm'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/encoders.html#RNNP"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.encoders.RNNP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<p>RNN with projection layer module</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idim</strong> (<em>int</em>) – dimension of inputs</p></li>
<li><p><strong>elayers</strong> (<em>int</em>) – number of encoder layers</p></li>
<li><p><strong>cdim</strong> (<em>int</em>) – number of rnn units (resulted in cdim * 2 if bidirectional)</p></li>
<li><p><strong>hdim</strong> (<em>int</em>) – number of projection units</p></li>
<li><p><strong>subsample</strong> (<em>np.ndarray</em>) – list of subsampling numbers</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout rate</p></li>
<li><p><strong>typ</strong> (<em>str</em>) – The RNN type</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.encoders.VGG2L">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.encoders.</code><code class="sig-name descname">VGG2L</code><span class="sig-paren">(</span><em class="sig-param">in_channel=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/encoders.html#VGG2L"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.encoders.VGG2L" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="function">
<dt id="espnet.nets.chainer_backend.encoders.encoder_for">
<code class="sig-prename descclassname">espnet.nets.chainer_backend.encoders.</code><code class="sig-name descname">encoder_for</code><span class="sig-paren">(</span><em class="sig-param">args</em>, <em class="sig-param">idim</em>, <em class="sig-param">subsample</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/encoders.html#encoder_for"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.encoders.encoder_for" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend-encoders-transformer">
<span id="id12"></span><h2>espnet.nets.chainer_backend.encoders_transformer<a class="headerlink" href="#espnet-nets-chainer-backend-encoders-transformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.encoders_transformer"></span><dl class="class">
<dt id="espnet.nets.chainer_backend.encoders_transformer.Conv2dSubsampling">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.encoders_transformer.</code><code class="sig-name descname">Conv2dSubsampling</code><span class="sig-paren">(</span><em class="sig-param">channels</em>, <em class="sig-param">idim</em>, <em class="sig-param">dims</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">initialW=None</em>, <em class="sig-param">initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/encoders_transformer.html#Conv2dSubsampling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.encoders_transformer.Conv2dSubsampling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.encoders_transformer.Encoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.encoders_transformer.</code><code class="sig-name descname">Encoder</code><span class="sig-paren">(</span><em class="sig-param">input_type</em>, <em class="sig-param">idim</em>, <em class="sig-param">n_layers</em>, <em class="sig-param">n_units</em>, <em class="sig-param">d_units=0</em>, <em class="sig-param">h=8</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">initialW=None</em>, <em class="sig-param">initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/encoders_transformer.html#Encoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.encoders_transformer.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.encoders_transformer.EncoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.encoders_transformer.</code><code class="sig-name descname">EncoderLayer</code><span class="sig-paren">(</span><em class="sig-param">n_units</em>, <em class="sig-param">d_units=0</em>, <em class="sig-param">h=8</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">initialW=None</em>, <em class="sig-param">initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/encoders_transformer.html#EncoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.encoders_transformer.EncoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.encoders_transformer.LinearSampling">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.encoders_transformer.</code><code class="sig-name descname">LinearSampling</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">dims</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">initialW=None</em>, <em class="sig-param">initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/encoders_transformer.html#LinearSampling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.encoders_transformer.LinearSampling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend-nets-utils">
<span id="id13"></span><h2>espnet.nets.chainer_backend.nets_utils<a class="headerlink" href="#espnet-nets-chainer-backend-nets-utils" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.nets_utils"></span><dl class="function">
<dt id="espnet.nets.chainer_backend.nets_utils.linear_tensor">
<code class="sig-prename descclassname">espnet.nets.chainer_backend.nets_utils.</code><code class="sig-name descname">linear_tensor</code><span class="sig-paren">(</span><em class="sig-param">linear</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/nets_utils.html#linear_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.nets_utils.linear_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply linear matrix operation only for the last dimension of a tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>linear</strong> (<em>Link</em>) – Linear link (M x N matrix)</p></li>
<li><p><strong>x</strong> (<em>Variable</em>) – Tensor (D_1 x D_2 x … x M matrix)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor (D_1 x D_2 x … x N matrix)</p>
</dd>
</dl>
<p>:rtype Variable</p>
</dd></dl>

</div>
<div class="section" id="espnet-nets-chainer-backend-nets-utils-transformer">
<span id="id14"></span><h2>espnet.nets.chainer_backend.nets_utils_transformer<a class="headerlink" href="#espnet-nets-chainer-backend-nets-utils-transformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.chainer_backend.nets_utils_transformer"></span><dl class="class">
<dt id="espnet.nets.chainer_backend.nets_utils_transformer.FeedForwardLayer">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.nets_utils_transformer.</code><code class="sig-name descname">FeedForwardLayer</code><span class="sig-paren">(</span><em class="sig-param">n_units</em>, <em class="sig-param">d_units=0</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">initialW=None</em>, <em class="sig-param">initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/nets_utils_transformer.html#FeedForwardLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.nets_utils_transformer.FeedForwardLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.nets_utils_transformer.LayerNorm">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.nets_utils_transformer.</code><code class="sig-name descname">LayerNorm</code><span class="sig-paren">(</span><em class="sig-param">dims</em>, <em class="sig-param">eps=1e-12</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/nets_utils_transformer.html#LayerNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.nets_utils_transformer.LayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.links.normalization.layer_normalization.LayerNormalization</span></code></p>
</dd></dl>

<dl class="class">
<dt id="espnet.nets.chainer_backend.nets_utils_transformer.PositionalEncoding">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.chainer_backend.nets_utils_transformer.</code><code class="sig-name descname">PositionalEncoding</code><span class="sig-paren">(</span><em class="sig-param">n_units</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">length=5000</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/nets_utils_transformer.html#PositionalEncoding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.nets_utils_transformer.PositionalEncoding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
</dd></dl>

<dl class="function">
<dt id="espnet.nets.chainer_backend.nets_utils_transformer.plot_multi_head_attention">
<code class="sig-prename descclassname">espnet.nets.chainer_backend.nets_utils_transformer.</code><code class="sig-name descname">plot_multi_head_attention</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">attn_dict</em>, <em class="sig-param">outdir</em>, <em class="sig-param">suffix='png'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/chainer_backend/nets_utils_transformer.html#plot_multi_head_attention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.chainer_backend.nets_utils_transformer.plot_multi_head_attention" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="espnet-nets-ctc-prefix-score">
<span id="id15"></span><h2>espnet.nets.ctc_prefix_score<a class="headerlink" href="#espnet-nets-ctc-prefix-score" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.ctc_prefix_score"></span><dl class="class">
<dt id="espnet.nets.ctc_prefix_score.CTCPrefixScore">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.ctc_prefix_score.</code><code class="sig-name descname">CTCPrefixScore</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">blank</em>, <em class="sig-param">eos</em>, <em class="sig-param">xp</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/ctc_prefix_score.html#CTCPrefixScore"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.ctc_prefix_score.CTCPrefixScore" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Compute CTC label sequence scores</p>
<p>which is based on Algorithm 2 in WATANABE et al.
“HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,”
but extended to efficiently compute the probablities of multiple labels
simultaneously</p>
<dl class="method">
<dt id="espnet.nets.ctc_prefix_score.CTCPrefixScore.initial_state">
<code class="sig-name descname">initial_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/ctc_prefix_score.html#CTCPrefixScore.initial_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.ctc_prefix_score.CTCPrefixScore.initial_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtain an initial CTC state</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>CTC state</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.ctc_prefix_score.CTCPrefixScoreTH">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.ctc_prefix_score.</code><code class="sig-name descname">CTCPrefixScoreTH</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">blank</em>, <em class="sig-param">eos</em>, <em class="sig-param">beam</em>, <em class="sig-param">hlens</em>, <em class="sig-param">device_id</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/ctc_prefix_score.html#CTCPrefixScoreTH"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.ctc_prefix_score.CTCPrefixScoreTH" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Batch processing of CTCPrefixScore</p>
<p>which is based on Algorithm 2 in WATANABE et al.
“HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,”
but extended to efficiently compute the probablities of multiple labels
simultaneously</p>
<dl class="method">
<dt id="espnet.nets.ctc_prefix_score.CTCPrefixScoreTH.initial_state">
<code class="sig-name descname">initial_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/ctc_prefix_score.html#CTCPrefixScoreTH.initial_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.ctc_prefix_score.CTCPrefixScoreTH.initial_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtain an initial CTC state</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>CTC state</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.ctc_prefix_score.CTCPrefixScoreTH.to_cuda">
<code class="sig-name descname">to_cuda</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/ctc_prefix_score.html#CTCPrefixScoreTH.to_cuda"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.ctc_prefix_score.CTCPrefixScoreTH.to_cuda" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet-nets-e2e-asr-common">
<span id="id16"></span><h2>espnet.nets.e2e_asr_common<a class="headerlink" href="#espnet-nets-e2e-asr-common" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.e2e_asr_common"></span><dl class="function">
<dt id="espnet.nets.e2e_asr_common.end_detect">
<code class="sig-prename descclassname">espnet.nets.e2e_asr_common.</code><code class="sig-name descname">end_detect</code><span class="sig-paren">(</span><em class="sig-param">ended_hyps</em>, <em class="sig-param">i</em>, <em class="sig-param">M=3</em>, <em class="sig-param">D_end=-10.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/e2e_asr_common.html#end_detect"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.e2e_asr_common.end_detect" title="Permalink to this definition">¶</a></dt>
<dd><p>End detection</p>
<p>desribed in Eq. (50) of S. Watanabe et al
“Hybrid CTC/Attention Architecture for End-to-End Speech Recognition”</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ended_hyps</strong> – </p></li>
<li><p><strong>i</strong> – </p></li>
<li><p><strong>M</strong> – </p></li>
<li><p><strong>D_end</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet.nets.e2e_asr_common.get_vgg2l_odim">
<code class="sig-prename descclassname">espnet.nets.e2e_asr_common.</code><code class="sig-name descname">get_vgg2l_odim</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">in_channel=3</em>, <em class="sig-param">out_channel=128</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/e2e_asr_common.html#get_vgg2l_odim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.e2e_asr_common.get_vgg2l_odim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="espnet.nets.e2e_asr_common.label_smoothing_dist">
<code class="sig-prename descclassname">espnet.nets.e2e_asr_common.</code><code class="sig-name descname">label_smoothing_dist</code><span class="sig-paren">(</span><em class="sig-param">odim</em>, <em class="sig-param">lsm_type</em>, <em class="sig-param">transcript=None</em>, <em class="sig-param">blank=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/e2e_asr_common.html#label_smoothing_dist"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.e2e_asr_common.label_smoothing_dist" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtain label distribution for loss smoothing</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>odim</strong> – </p></li>
<li><p><strong>lsm_type</strong> – </p></li>
<li><p><strong>blank</strong> – </p></li>
<li><p><strong>transcript</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="espnet-nets-pytorch-backend">
<span id="id17"></span><h2>espnet.nets.pytorch_backend<a class="headerlink" href="#espnet-nets-pytorch-backend" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend"></span></div>
<div class="section" id="espnet-nets-pytorch-backend-ctc">
<span id="id18"></span><h2>espnet.nets.pytorch_backend.ctc<a class="headerlink" href="#espnet-nets-pytorch-backend-ctc" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.ctc"></span><dl class="class">
<dt id="espnet.nets.pytorch_backend.ctc.CTC">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.ctc.</code><code class="sig-name descname">CTC</code><span class="sig-paren">(</span><em class="sig-param">odim</em>, <em class="sig-param">eprojs</em>, <em class="sig-param">dropout_rate</em>, <em class="sig-param">ctc_type='warpctc'</em>, <em class="sig-param">reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/ctc.html#CTC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.ctc.CTC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>CTC module</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>odim</strong> (<em>int</em>) – dimension of outputs</p></li>
<li><p><strong>eprojs</strong> (<em>int</em>) – number of encoder projection units</p></li>
<li><p><strong>dropout_rate</strong> (<em>float</em>) – dropout rate (0.0 ~ 1.0)</p></li>
<li><p><strong>ctc_type</strong> (<em>str</em>) – builtin or warpctc</p></li>
<li><p><strong>reduce</strong> (<em>bool</em>) – reduce the CTC loss into a scalar</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.ctc.CTC.argmax">
<code class="sig-name descname">argmax</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/ctc.html#CTC.argmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.ctc.CTC.argmax" title="Permalink to this definition">¶</a></dt>
<dd><p>argmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs_pad</strong> (<em>torch.Tensor</em>) – 3d tensor (B, Tmax, eprojs)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>argmax applied 2d tensor (B, Tmax)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.ctc.CTC.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em>, <em class="sig-param">hlens</em>, <em class="sig-param">ys_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/ctc.html#CTC.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.ctc.CTC.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>CTC forward</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hs_pad</strong> (<em>torch.Tensor</em>) – batch of padded hidden state sequences (B, Tmax, D)</p></li>
<li><p><strong>hlens</strong> (<em>torch.Tensor</em>) – batch of lengths of hidden state sequences (B)</p></li>
<li><p><strong>ys_pad</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, Lmax)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>ctc loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.ctc.CTC.log_softmax">
<code class="sig-name descname">log_softmax</code><span class="sig-paren">(</span><em class="sig-param">hs_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/ctc.html#CTC.log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.ctc.CTC.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>log_softmax of frame activations</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hs_pad</strong> (<em>torch.Tensor</em>) – 3d tensor (B, Tmax, eprojs)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>log softmax applied 3d tensor (B, Tmax, odim)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.ctc.CTC.loss_fn">
<code class="sig-name descname">loss_fn</code><span class="sig-paren">(</span><em class="sig-param">th_pred</em>, <em class="sig-param">th_target</em>, <em class="sig-param">th_ilen</em>, <em class="sig-param">th_olen</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/ctc.html#CTC.loss_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.ctc.CTC.loss_fn" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet.nets.pytorch_backend.ctc.ctc_for">
<code class="sig-prename descclassname">espnet.nets.pytorch_backend.ctc.</code><code class="sig-name descname">ctc_for</code><span class="sig-paren">(</span><em class="sig-param">args</em>, <em class="sig-param">odim</em>, <em class="sig-param">reduce=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/ctc.html#ctc_for"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.ctc.ctc_for" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the CTC module for the given args and output dimension</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> (<em>Namespace</em>) – the program args</p>
</dd>
</dl>
<p>:param int odim : The output dimension
:param bool reduce : return the CTC loss in a scalar
:return: the corresponding CTC module</p>
</dd></dl>

</div>
<div class="section" id="espnet-nets-pytorch-backend-e2e-asr">
<span id="id19"></span><h2>espnet.nets.pytorch_backend.e2e_asr<a class="headerlink" href="#espnet-nets-pytorch-backend-e2e-asr" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.e2e_asr"></span><dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_asr.E2E">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_asr.</code><code class="sig-name descname">E2E</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">odim</em>, <em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr.html#E2E"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr.E2E" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet.nets.asr_interface.ASRInterface" title="espnet.nets.asr_interface.ASRInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.asr_interface.ASRInterface</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>E2E module</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idim</strong> (<em>int</em>) – dimension of inputs</p></li>
<li><p><strong>odim</strong> (<em>int</em>) – dimension of outputs</p></li>
<li><p><strong>args</strong> (<em>Namespace</em>) – argument Namespace containing options</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr.E2E.calculate_all_attentions">
<code class="sig-name descname">calculate_all_attentions</code><span class="sig-paren">(</span><em class="sig-param">xs_pad</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr.html#E2E.calculate_all_attentions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr.E2E.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E attention calculation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, idim)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</p></li>
<li><p><strong>ys_pad</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, Lmax)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>attention weights with the following shape,
1) multi-head case =&gt; attention weights (B, H, Lmax, Tmax),
2) other case =&gt; attention weights (B, Lmax, Tmax).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr.E2E.enhance">
<code class="sig-name descname">enhance</code><span class="sig-paren">(</span><em class="sig-param">xs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr.html#E2E.enhance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr.E2E.enhance" title="Permalink to this definition">¶</a></dt>
<dd><p>Forwarding only the frontend stage</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>xs</strong> (<em>ndarray</em>) – input acoustic feature (T, C, F)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr.E2E.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr.html#E2E.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr.E2E.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E forward</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, idim)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</p></li>
<li><p><strong>ys_pad</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, Lmax)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>ctc loass value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>attention loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>accuracy in attention decoder</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr.E2E.init_like_chainer">
<code class="sig-name descname">init_like_chainer</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr.html#E2E.init_like_chainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr.E2E.init_like_chainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize weight like chainer</p>
<p>chainer basically uses LeCun way: W ~ Normal(0, fan_in ** -0.5), b = 0
pytorch basically uses W, b ~ Uniform(-fan_in**-0.5, fan_in**-0.5)</p>
<p>however, there are two exceptions as far as I know.
- EmbedID.W ~ Normal(0, 1)
- LSTM.upward.b[forget_gate_range] = 1 (but not used in NStepLSTM)</p>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr.E2E.recognize">
<code class="sig-name descname">recognize</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">recog_args</em>, <em class="sig-param">char_list</em>, <em class="sig-param">rnnlm=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr.html#E2E.recognize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr.E2E.recognize" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E beam search</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>ndarray</em>) – input acoustic feature (T, D)</p></li>
<li><p><strong>recog_args</strong> (<em>Namespace</em>) – argument Namespace containing options</p></li>
<li><p><strong>char_list</strong> (<em>list</em>) – list of characters</p></li>
<li><p><strong>rnnlm</strong> (<em>torch.nn.Module</em>) – language model module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best decoding results</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr.E2E.recognize_batch">
<code class="sig-name descname">recognize_batch</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">recog_args</em>, <em class="sig-param">char_list</em>, <em class="sig-param">rnnlm=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr.html#E2E.recognize_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr.E2E.recognize_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E beam search</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>list</em>) – list of input acoustic feature arrays [(T_1, D), (T_2, D), …]</p></li>
<li><p><strong>recog_args</strong> (<em>Namespace</em>) – argument Namespace containing options</p></li>
<li><p><strong>char_list</strong> (<em>list</em>) – list of characters</p></li>
<li><p><strong>rnnlm</strong> (<em>torch.nn.Module</em>) – language model module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best decoding results</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr.E2E.subsample_frames">
<code class="sig-name descname">subsample_frames</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr.html#E2E.subsample_frames"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr.E2E.subsample_frames" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_asr.Reporter">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_asr.</code><code class="sig-name descname">Reporter</code><span class="sig-paren">(</span><em class="sig-param">**links</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr.html#Reporter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr.Reporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<p>A chainer reporter wrapper</p>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr.Reporter.report">
<code class="sig-name descname">report</code><span class="sig-paren">(</span><em class="sig-param">loss_ctc</em>, <em class="sig-param">loss_att</em>, <em class="sig-param">acc</em>, <em class="sig-param">cer_ctc</em>, <em class="sig-param">cer</em>, <em class="sig-param">wer</em>, <em class="sig-param">mtl_loss</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr.html#Reporter.report"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr.Reporter.report" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet-nets-pytorch-backend-e2e-asr-mix">
<span id="id20"></span><h2>espnet.nets.pytorch_backend.e2e_asr_mix<a class="headerlink" href="#espnet-nets-pytorch-backend-e2e-asr-mix" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.e2e_asr_mix"></span><dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.E2E">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_asr_mix.</code><code class="sig-name descname">E2E</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">odim</em>, <em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#E2E"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.E2E" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet.nets.asr_interface.ASRInterface" title="espnet.nets.asr_interface.ASRInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.asr_interface.ASRInterface</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>E2E module</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idim</strong> (<em>int</em>) – dimension of inputs</p></li>
<li><p><strong>odim</strong> (<em>int</em>) – dimension of outputs</p></li>
<li><p><strong>args</strong> (<em>Namespace</em>) – argument Namespace containing options</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.E2E.calculate_all_attentions">
<code class="sig-name descname">calculate_all_attentions</code><span class="sig-paren">(</span><em class="sig-param">xs_pad</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys_pad_sd</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#E2E.calculate_all_attentions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.E2E.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E attention calculation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, idim)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</p></li>
<li><p><strong>ys_pad_sd</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, num_spkrs, Lmax)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>attention weights with the following shape,
1) multi-head case =&gt; attention weights (B, H, Lmax, Tmax),
2) other case =&gt; attention weights (B, Lmax, Tmax).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.E2E.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys_pad_sd</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#E2E.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.E2E.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E forward</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, idim)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</p></li>
<li><p><strong>ys_pad_sd</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, num_spkrs, Lmax)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>ctc loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>attention loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>accuracy in attention decoder</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.E2E.init_like_chainer">
<code class="sig-name descname">init_like_chainer</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#E2E.init_like_chainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.E2E.init_like_chainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize weight like chainer</p>
<p>chainer basically uses LeCun way: W ~ Normal(0, fan_in ** -0.5), b = 0
pytorch basically uses W, b ~ Uniform(-fan_in**-0.5, fan_in**-0.5)</p>
<p>however, there are two exceptions as far as I know.
- EmbedID.W ~ Normal(0, 1)
- LSTM.upward.b[forget_gate_range] = 1 (but not used in NStepLSTM)</p>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.E2E.recognize">
<code class="sig-name descname">recognize</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">recog_args</em>, <em class="sig-param">char_list</em>, <em class="sig-param">rnnlm=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#E2E.recognize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.E2E.recognize" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E beam search</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>ndarray</em>) – input acoustic feature (T, D)</p></li>
<li><p><strong>recog_args</strong> (<em>Namespace</em>) – argument Namespace containing options</p></li>
<li><p><strong>char_list</strong> (<em>list</em>) – list of characters</p></li>
<li><p><strong>rnnlm</strong> (<em>torch.nn.Module</em>) – language model module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best decoding results</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.E2E.recognize_batch">
<code class="sig-name descname">recognize_batch</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">recog_args</em>, <em class="sig-param">char_list</em>, <em class="sig-param">rnnlm=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#E2E.recognize_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.E2E.recognize_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E beam search</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>ndarray</em>) – input acoustic feature (T, D)</p></li>
<li><p><strong>recog_args</strong> (<em>Namespace</em>) – argument Namespace containing options</p></li>
<li><p><strong>char_list</strong> (<em>list</em>) – list of characters</p></li>
<li><p><strong>rnnlm</strong> (<em>torch.nn.Module</em>) – language model module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best decoding results</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.Encoder">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_asr_mix.</code><code class="sig-name descname">Encoder</code><span class="sig-paren">(</span><em class="sig-param">etype</em>, <em class="sig-param">idim</em>, <em class="sig-param">elayers_sd</em>, <em class="sig-param">elayers_rec</em>, <em class="sig-param">eunits</em>, <em class="sig-param">eprojs</em>, <em class="sig-param">subsample</em>, <em class="sig-param">dropout</em>, <em class="sig-param">num_spkrs=2</em>, <em class="sig-param">in_channel=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#Encoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Encoder module</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>etype</strong> (<em>str</em>) – type of encoder network</p></li>
<li><p><strong>idim</strong> (<em>int</em>) – number of dimensions of encoder network</p></li>
<li><p><strong>elayers_sd</strong> (<em>int</em>) – number of layers of speaker differentiate part in encoder network</p></li>
<li><p><strong>elayers_rec</strong> (<em>int</em>) – number of layers of shared recognition part in encoder network</p></li>
<li><p><strong>eunits</strong> (<em>int</em>) – number of lstm units of encoder network</p></li>
<li><p><strong>eprojs</strong> (<em>int</em>) – number of projection units of encoder network</p></li>
<li><p><strong>subsample</strong> (<em>np.ndarray</em>) – list of subsampling numbers</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout rate</p></li>
<li><p><strong>in_channel</strong> (<em>int</em>) – number of input channels</p></li>
<li><p><strong>num_spkrs</strong> (<em>int</em>) – number of number of speakers</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.Encoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad</em>, <em class="sig-param">ilens</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#Encoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.Encoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Encoder forward</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, D)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list: batch of hidden state sequences [num_spkrs x (B, Tmax, eprojs)]</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.PIT">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_asr_mix.</code><code class="sig-name descname">PIT</code><span class="sig-paren">(</span><em class="sig-param">num_spkrs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#PIT"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.PIT" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Permutation Invariant Training (PIT) module</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>num_spkrs</strong> (<em>int</em>) – number of speakers for PIT process (2 or 3)</p>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.PIT.min_pit_sample">
<code class="sig-name descname">min_pit_sample</code><span class="sig-paren">(</span><em class="sig-param">loss</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#PIT.min_pit_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.PIT.min_pit_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>PIT min_pit_sample</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>torch.Tensor loss</strong> (<em>1-D</em>) – list of losses for one sample,
including [h1r1, h1r2, h2r1, h2r2] or [h1r1, h1r2, h1r3, h2r1, h2r2, h2r3, h3r1, h3r2, h3r3]</p>
</dd>
</dl>
<p>:return min_loss
:rtype torch.Tensor (1)
:return permutation
:rtype List: len=2</p>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.PIT.pit_process">
<code class="sig-name descname">pit_process</code><span class="sig-paren">(</span><em class="sig-param">losses</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#PIT.pit_process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.PIT.pit_process" title="Permalink to this definition">¶</a></dt>
<dd><p>PIT pit_process</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>losses</strong> (<em>torch.Tensor</em>) – losses (B, 1|4|9)</p>
</dd>
</dl>
<p>:return pit_loss
:rtype torch.Tensor (B)
:return permutation
:rtype torch.LongTensor (B, 1|2|3)</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.Reporter">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_asr_mix.</code><code class="sig-name descname">Reporter</code><span class="sig-paren">(</span><em class="sig-param">**links</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#Reporter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.Reporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<p>A chainer reporter wrapper</p>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.Reporter.report">
<code class="sig-name descname">report</code><span class="sig-paren">(</span><em class="sig-param">loss_ctc</em>, <em class="sig-param">loss_att</em>, <em class="sig-param">acc</em>, <em class="sig-param">cer</em>, <em class="sig-param">wer</em>, <em class="sig-param">mtl_loss</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#Reporter.report"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.Reporter.report" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet.nets.pytorch_backend.e2e_asr_mix.encoder_for">
<code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_asr_mix.</code><code class="sig-name descname">encoder_for</code><span class="sig-paren">(</span><em class="sig-param">args</em>, <em class="sig-param">idim</em>, <em class="sig-param">subsample</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_mix.html#encoder_for"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_mix.encoder_for" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="espnet-nets-pytorch-backend-e2e-asr-transformer">
<span id="id21"></span><h2>espnet.nets.pytorch_backend.e2e_asr_transformer<a class="headerlink" href="#espnet-nets-pytorch-backend-e2e-asr-transformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.e2e_asr_transformer"></span><dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_asr_transformer.E2E">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_asr_transformer.</code><code class="sig-name descname">E2E</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">odim</em>, <em class="sig-param">args</em>, <em class="sig-param">ignore_id=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_transformer.html#E2E"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_transformer.E2E" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet.nets.asr_interface.ASRInterface" title="espnet.nets.asr_interface.ASRInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.asr_interface.ASRInterface</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.add_arguments">
<em class="property">static </em><code class="sig-name descname">add_arguments</code><span class="sig-paren">(</span><em class="sig-param">parser</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_transformer.html#E2E.add_arguments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.add_arguments" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.add_sos_eos">
<code class="sig-name descname">add_sos_eos</code><span class="sig-paren">(</span><em class="sig-param">ys_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_transformer.html#E2E.add_sos_eos"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.add_sos_eos" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.attention_plot_class">
<em class="property">property </em><code class="sig-name descname">attention_plot_class</code><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.attention_plot_class" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.calculate_all_attentions">
<code class="sig-name descname">calculate_all_attentions</code><span class="sig-paren">(</span><em class="sig-param">xs_pad</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_transformer.html#E2E.calculate_all_attentions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E attention calculation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded input sequences (B, Tmax, idim)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of input sequences (B)</p></li>
<li><p><strong>ys_pad</strong> (<em>torch.Tensor</em>) – batch of padded character id sequence tensor (B, Lmax)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>attention weights with the following shape,
1) multi-head case =&gt; attention weights (B, H, Lmax, Tmax),
2) other case =&gt; attention weights (B, Lmax, Tmax).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs_pad</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_transformer.html#E2E.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>E2E forward</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs_pad</strong> (<em>torch.Tensor</em>) – batch of padded source sequences (B, Tmax, idim)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – batch of lengths of source sequences (B)</p></li>
<li><p><strong>ys_pad</strong> (<em>torch.Tensor</em>) – batch of padded target sequences (B, Lmax)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>ctc loass value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>attention loss value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>accuracy in attention decoder</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.recognize">
<code class="sig-name descname">recognize</code><span class="sig-paren">(</span><em class="sig-param">feat</em>, <em class="sig-param">recog_args</em>, <em class="sig-param">char_list=None</em>, <em class="sig-param">rnnlm=None</em>, <em class="sig-param">use_jit=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_transformer.html#E2E.recognize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.recognize" title="Permalink to this definition">¶</a></dt>
<dd><p>recognize feat</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>ndnarray</em>) – input acouctic feature (B, T, D) or (T, D)</p></li>
<li><p><strong>recog_args</strong> (<em>namespace</em>) – argment namespace contraining options</p></li>
<li><p><strong>char_list</strong> (<em>list</em>) – list of characters</p></li>
<li><p><strong>rnnlm</strong> (<em>torch.nn.Module</em>) – language model module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-best decoding results</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
<p>TODO(karita): do not recompute previous attention for faster decoding</p>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.reset_parameters">
<code class="sig-name descname">reset_parameters</code><span class="sig-paren">(</span><em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_transformer.html#E2E.reset_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.target_mask">
<code class="sig-name descname">target_mask</code><span class="sig-paren">(</span><em class="sig-param">ys_in_pad</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_transformer.html#E2E.target_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_transformer.E2E.target_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="espnet.nets.pytorch_backend.e2e_asr_transformer.subsequent_mask">
<code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_asr_transformer.</code><code class="sig-name descname">subsequent_mask</code><span class="sig-paren">(</span><em class="sig-param">size</em>, <em class="sig-param">device='cpu'</em>, <em class="sig-param">dtype=torch.uint8</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_asr_transformer.html#subsequent_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_asr_transformer.subsequent_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Create mask for subsequent steps (1, size, size)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em>) – size of mask</p></li>
<li><p><strong>device</strong> (<em>str</em>) – “cpu” or “cuda” or torch.Tensor.device</p></li>
<li><p><strong>dtype</strong> (<em>torch.dtype</em>) – result dtype</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>torch.Tensor</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">subsequent_mask</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="go">[[1, 0, 0],</span>
<span class="go"> [1, 1, 0],</span>
<span class="go"> [1, 1, 1]]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="espnet-nets-pytorch-backend-e2e-tts-fastspeech">
<span id="id22"></span><h2>espnet.nets.pytorch_backend.e2e_tts_fastspeech<a class="headerlink" href="#espnet-nets-pytorch-backend-e2e-tts-fastspeech" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.e2e_tts_fastspeech"></span><dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_tts_fastspeech.</code><code class="sig-name descname">FeedForwardTransformer</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">odim</em>, <em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_fastspeech.html#FeedForwardTransformer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet.nets.tts_interface.TTSInterface" title="espnet.nets.tts_interface.TTSInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.tts_interface.TTSInterface</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Feed Forward Transformer for TTS a.k.a. FastSpeech.</p>
<p>This is a module of FastSpeech, feed-forward Transformer with duration predictor described in
<a class="reference external" href="https://arxiv.org/pdf/1905.09263.pdf">FastSpeech: Fast, Robust and Controllable Text to Speech</a>, which does not require any auto-regressive
processing during inference, resulting in fast decoding compared with auto-regressive Transformer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idim</strong> (<em>int</em>) – Dimension of the inputs.</p></li>
<li><p><strong>odim</strong> (<em>int</em>) – Dimension of the outputs.</p></li>
<li><p><strong>args</strong> (<em>Namespace</em>) – <ul>
<li><p>elayers (int): Number of encoder layers.</p></li>
<li><p>eunits (int): Number of encoder hidden units.</p></li>
<li><p>adim (int): Number of attention transformation dimensions.</p></li>
<li><p>aheads (int): Number of heads for multi head attention.</p></li>
<li><p>dlayers (int): Number of decoder layers.</p></li>
<li><p>dunits (int): Number of decoder hidden units.</p></li>
<li><p>use_scaled_pos_enc (bool): Whether to use trainable scaled positional encoding.</p></li>
<li><p>encoder_normalize_before (bool): Whether to perform layer normalization before encoder block.</p></li>
<li><p>decoder_normalize_before (bool): Whether to perform layer normalization before decoder block.</p></li>
<li><p>encoder_concat_after (bool): Whether to concatenate attention layer’s input and output in encoder.</p></li>
<li><p>decoder_concat_after (bool): Whether to concatenate attention layer’s input and output in decoder.</p></li>
<li><p>duration_predictor_layers (int): Number of duration predictor layers.</p></li>
<li><p>duration_predictor_chans (int): Number of duration predictor channels.</p></li>
<li><p>duration_predictor_kernel_size (int): Kernel size of duration predictor.</p></li>
<li><p>teacher_model (str): Teacher auto-regressive transformer model path.</p></li>
<li><p>reduction_factor (int): Reduction factor.</p></li>
<li><p>transformer_init (float): How to initialize transformer parameters.</p></li>
<li><p>transformer_lr (float): Initial value of learning rate.</p></li>
<li><p>transformer_warmup_steps (int): Optimizer warmup steps.</p></li>
<li><p>transformer_enc_dropout_rate (float): Dropout rate in encoder except for attention &amp; positional encoding.</p></li>
<li><p>transformer_enc_positional_dropout_rate (float): Dropout rate after encoder positional encoding.</p></li>
<li><p>transformer_enc_attn_dropout_rate (float): Dropout rate in encoder self-attention module.</p></li>
<li><p>transformer_dec_dropout_rate (float): Dropout rate in decoder except for attention &amp; positional encoding.</p></li>
<li><p>transformer_dec_positional_dropout_rate (float): Dropout rate after decoder positional encoding.</p></li>
<li><p>transformer_dec_attn_dropout_rate (float): Dropout rate in deocoder self-attention module.</p></li>
<li><p>transformer_enc_dec_attn_dropout_rate (float): Dropout rate in encoder-deocoder attention module.</p></li>
<li><p>use_masking (bool): Whether to use masking in calculation of loss.</p></li>
<li><p>transfer_encoder_from_teacher: Whether to transfer encoder using teacher encoder parameters.</p></li>
<li><p>transferred_encoder_module: Encoder module to be initialized using teacher parameters.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.add_arguments">
<em class="property">static </em><code class="sig-name descname">add_arguments</code><span class="sig-paren">(</span><em class="sig-param">parser</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_fastspeech.html#FeedForwardTransformer.add_arguments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.add_arguments" title="Permalink to this definition">¶</a></dt>
<dd><p>Add model-specific arguments to the parser.</p>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.attention_plot_class">
<em class="property">property </em><code class="sig-name descname">attention_plot_class</code><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.attention_plot_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Return plot class for attention weight plot.</p>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.base_plot_keys">
<em class="property">property </em><code class="sig-name descname">base_plot_keys</code><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.base_plot_keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Return base key names to plot during training. keys should match what <cite>chainer.reporter</cite> reports.</p>
<p>If you add the key <cite>loss</cite>, the reporter will report <cite>main/loss</cite> and <cite>validation/main/loss</cite> values.
also <cite>loss.png</cite> will be created as a figure visulizing <cite>main/loss</cite> and <cite>validation/main/loss</cite> values.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of strings which are base keys to plot during training.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.calculate_all_attentions">
<code class="sig-name descname">calculate_all_attentions</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em>, <em class="sig-param">olens</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_fastspeech.html#FeedForwardTransformer.calculate_all_attentions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate attention weights</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>torch.Tensor</em>) – batch of padded character ids (B, Tmax)</p></li>
<li><p><strong>ilens</strong> (<em>torch.Tensor</em>) – list of lengths of each input batch (B)</p></li>
<li><p><strong>ys</strong> (<em>torch.Tensor</em>) – batch of padded target features (B, Lmax, odim)</p></li>
<li><p><strong>ilens</strong> – list of lengths of each output batch (B)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>attention weights dict</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em>, <em class="sig-param">olens</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_fastspeech.html#FeedForwardTransformer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>Tensor</em>) – Batch of padded character ids (B, Tmax).</p></li>
<li><p><strong>ilens</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</p></li>
<li><p><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</p></li>
<li><p><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.inference">
<code class="sig-name descname">inference</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_fastspeech.html#FeedForwardTransformer.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_fastspeech.FeedForwardTransformer.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate the sequence of features given the sequences of characters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – Input sequence of characters (T,).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output sequence of features (1, L, odim).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet-nets-pytorch-backend-e2e-tts-tacotron2">
<span id="id23"></span><h2>espnet.nets.pytorch_backend.e2e_tts_tacotron2<a class="headerlink" href="#espnet-nets-pytorch-backend-e2e-tts-tacotron2" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.e2e_tts_tacotron2"></span><dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.CBHGLoss">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_tts_tacotron2.</code><code class="sig-name descname">CBHGLoss</code><span class="sig-paren">(</span><em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_tacotron2.html#CBHGLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.CBHGLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Loss function module for CBHG.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> (<em>Namespace</em>) – <ul class="simple">
<li><p>use_masking (bool): Whether to mask padded part in loss calculation</p></li>
</ul>
</p>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.CBHGLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">cbhg_outs</em>, <em class="sig-param">spcs</em>, <em class="sig-param">olens</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_tacotron2.html#CBHGLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.CBHGLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cbhg_outs</strong> (<em>Tensor</em>) – Batch of CBHG outputs (B, Lmax, spc_dim).</p></li>
<li><p><strong>spcs</strong> (<em>Tensor</em>) – Batch of groundtruth of spectrogram (B, Lmax, spc_dim).</p></li>
<li><p><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each sequence (B,).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>L1 loss value
Tensor: Mean square error loss value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_tts_tacotron2.</code><code class="sig-name descname">GuidedAttentionLoss</code><span class="sig-paren">(</span><em class="sig-param">sigma=0.4</em>, <em class="sig-param">reset_always=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_tacotron2.html#GuidedAttentionLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Guided attention loss function module.</p>
<p>This module calculates the guided attention loss described in <a class="reference external" href="https://arxiv.org/abs/1710.08969">Efficiently Trainable Text-to-Speech System Based
on Deep Convolutional Networks with Guided Attention</a>, which forces the attention to be diagonal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sigma</strong> (<em>float</em><em>, </em><em>optional</em>) – Standard deviation to control how close attention to a diagonal.</p></li>
<li><p><strong>reset_always</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to always reset masks.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">att_ws</em>, <em class="sig-param">ilens</em>, <em class="sig-param">olens</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_tacotron2.html#GuidedAttentionLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>att_ws</strong> (<em>Tensor</em>) – Batch of attention weights (B, T_max_out, T_max_in).</p></li>
<li><p><strong>ilens</strong> (<em>LongTensor</em>) – Batch of input lenghts (B,).</p></li>
<li><p><strong>olens</strong> (<em>LongTensor</em>) – Batch of output lenghts (B,).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Guided attention loss value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_tts_tacotron2.</code><code class="sig-name descname">Tacotron2</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">odim</em>, <em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_tacotron2.html#Tacotron2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet.nets.tts_interface.TTSInterface" title="espnet.nets.tts_interface.TTSInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.tts_interface.TTSInterface</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Tacotron2 module for end-to-end text-to-speech (E2E-TTS).</p>
<p>This is a module of Spectrogram prediction network in Tacotron2 described in <a class="reference external" href="https://arxiv.org/abs/1712.05884">Natural TTS Synthesis
by Conditioning WaveNet on Mel Spectrogram Predictions</a>, which converts the sequence of characters
into the sequence of Mel-filterbanks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idim</strong> (<em>int</em>) – Dimension of the inputs.</p></li>
<li><p><strong>odim</strong> (<em>int</em>) – Dimension of the outputs.</p></li>
<li><p><strong>args</strong> (<em>Namespace</em>) – <ul>
<li><p>spk_embed_dim (int): Dimension of the speaker embedding.</p></li>
<li><p>embed_dim (int): Dimension of character embedding.</p></li>
<li><p>elayers (int): The number of encoder blstm layers.</p></li>
<li><p>eunits (int): The number of encoder blstm units.</p></li>
<li><p>econv_layers (int): The number of encoder conv layers.</p></li>
<li><p>econv_filts (int): The number of encoder conv filter size.</p></li>
<li><p>econv_chans (int): The number of encoder conv filter channels.</p></li>
<li><p>dlayers (int): The number of decoder lstm layers.</p></li>
<li><p>dunits (int): The number of decoder lstm units.</p></li>
<li><p>prenet_layers (int): The number of prenet layers.</p></li>
<li><p>prenet_units (int): The number of prenet units.</p></li>
<li><p>postnet_layers (int): The number of postnet layers.</p></li>
<li><p>postnet_filts (int): The number of postnet filter size.</p></li>
<li><p>postnet_chans (int): The number of postnet filter channels.</p></li>
<li><p>output_activation (int): The name of activation function for outputs.</p></li>
<li><p>adim (int): The number of dimension of mlp in attention.</p></li>
<li><p>aconv_chans (int): The number of attention conv filter channels.</p></li>
<li><p>aconv_filts (int): The number of attention conv filter size.</p></li>
<li><p>cumulate_att_w (bool): Whether to cumulate previous attention weight.</p></li>
<li><p>use_batch_norm (bool): Whether to use batch normalization.</p></li>
<li><p>use_concate (int): Whether to concatenate encoder embedding with decoder lstm outputs.</p></li>
<li><p>dropout_rate (float): Dropout rate.</p></li>
<li><p>zoneout_rate (float): Zoneout rate.</p></li>
<li><p>reduction_factor (int): Reduction factor.</p></li>
<li><p>use_cbhg (bool): Whether to use CBHG module.</p></li>
<li><p>cbhg_conv_bank_layers (int): The number of convoluional banks in CBHG.</p></li>
<li><p>cbhg_conv_bank_chans (int): The number of channels of convolutional bank in CBHG.</p></li>
<li><p>cbhg_proj_filts (int): The number of filter size of projection layeri in CBHG.</p></li>
<li><p>cbhg_proj_chans (int): The number of channels of projection layer in CBHG.</p></li>
<li><p>cbhg_highway_layers (int): The number of layers of highway network in CBHG.</p></li>
<li><p>cbhg_highway_units (int): The number of units of highway network in CBHG.</p></li>
<li><p>cbhg_gru_units (int): The number of units of GRU in CBHG.</p></li>
<li><p>use_masking (bool): Whether to mask padded part in loss calculation.</p></li>
<li><p>bce_pos_weight (float): Weight of positive sample of stop token (only for use_masking=True).</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2.add_arguments">
<em class="property">static </em><code class="sig-name descname">add_arguments</code><span class="sig-paren">(</span><em class="sig-param">parser</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_tacotron2.html#Tacotron2.add_arguments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2.add_arguments" title="Permalink to this definition">¶</a></dt>
<dd><p>Add model-specific arguments to the parser.</p>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2.base_plot_keys">
<em class="property">property </em><code class="sig-name descname">base_plot_keys</code><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2.base_plot_keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Return base key names to plot during training. keys should match what <cite>chainer.reporter</cite> reports.</p>
<p>If you add the key <cite>loss</cite>, the reporter will report <cite>main/loss</cite> and <cite>validation/main/loss</cite> values.
also <cite>loss.png</cite> will be created as a figure visulizing <cite>main/loss</cite> and <cite>validation/main/loss</cite> values.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of strings which are base keys to plot during training.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2.calculate_all_attentions">
<code class="sig-name descname">calculate_all_attentions</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em>, <em class="sig-param">spembs=None</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_tacotron2.html#Tacotron2.calculate_all_attentions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate all of the attention weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>Tensor</em>) – Batch of padded character ids (B, Tmax).</p></li>
<li><p><strong>ilens</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</p></li>
<li><p><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</p></li>
<li><p><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</p></li>
<li><p><strong>spembs</strong> (<em>Tensor</em><em>, </em><em>optional</em>) – Batch of speaker embedding vectors (B, spk_embed_dim).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Batch of attention weights (B, Lmax, Tmax).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em>, <em class="sig-param">labels</em>, <em class="sig-param">olens</em>, <em class="sig-param">spembs=None</em>, <em class="sig-param">spcs=None</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_tacotron2.html#Tacotron2.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>Tensor</em>) – Batch of padded character ids (B, Tmax).</p></li>
<li><p><strong>ilens</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</p></li>
<li><p><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</p></li>
<li><p><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</p></li>
<li><p><strong>spembs</strong> (<em>Tensor</em><em>, </em><em>optional</em>) – Batch of speaker embedding vectors (B, spk_embed_dim).</p></li>
<li><p><strong>spcs</strong> (<em>Tensor</em><em>, </em><em>optional</em>) – Batch of groundtruth spectrograms (B, Lmax, spc_dim).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2.inference">
<code class="sig-name descname">inference</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">inference_args</em>, <em class="sig-param">spemb=None</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_tacotron2.html#Tacotron2.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate the sequence of features given the sequences of characters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – Input sequence of characters (T,).</p></li>
<li><p><strong>inference_args</strong> (<em>Namespace</em>) – <ul>
<li><p>threshold (float): Threshold in inference.</p></li>
<li><p>minlenratio (float): Minimum length ratio in inference.</p></li>
<li><p>maxlenratio (float): Maximum length ratio in inference.</p></li>
</ul>
</p></li>
<li><p><strong>spemb</strong> (<em>Tensor</em><em>, </em><em>optional</em>) – Speaker embedding vector (spk_embed_dim).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output sequence of features (L, odim).
Tensor: Output sequence of stop probabilities (L,).
Tensor: Attention weights (L, T).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2Loss">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_tts_tacotron2.</code><code class="sig-name descname">Tacotron2Loss</code><span class="sig-paren">(</span><em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_tacotron2.html#Tacotron2Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Loss function module for Tacotron2.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> (<em>Namespace</em>) – <ul class="simple">
<li><p>use_masking (bool): Whether to mask padded part in loss calculation.</p></li>
<li><p>bce_pos_weight (float): Weight of positive sample of stop token (only for use_masking=True).</p></li>
</ul>
</p>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2Loss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">after_outs</em>, <em class="sig-param">before_outs</em>, <em class="sig-param">logits</em>, <em class="sig-param">ys</em>, <em class="sig-param">labels</em>, <em class="sig-param">olens</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_tacotron2.html#Tacotron2Loss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.Tacotron2Loss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>after_outs</strong> (<em>Tensor</em>) – Batch of outputs after postnets (B, Lmax, odim).</p></li>
<li><p><strong>before_outs</strong> (<em>Tensor</em>) – Batch of outputs before postnets (B, Lmax, odim).</p></li>
<li><p><strong>logits</strong> (<em>Tensor</em>) – Batch of stop logits (B, Lmax).</p></li>
<li><p><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</p></li>
<li><p><strong>labels</strong> (<em>LongTensor</em>) – Batch of the sequences of stop token labels (B, Lmax).</p></li>
<li><p><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>L1 loss value.
Tensor: Mean square error loss value.
Tensor: Binary cross entropy loss value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet-nets-pytorch-backend-e2e-tts-transformer">
<span id="id24"></span><h2>espnet.nets.pytorch_backend.e2e_tts_transformer<a class="headerlink" href="#espnet-nets-pytorch-backend-e2e-tts-transformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.e2e_tts_transformer"></span><dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.GuidedMultiHeadAttentionLoss">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_tts_transformer.</code><code class="sig-name descname">GuidedMultiHeadAttentionLoss</code><span class="sig-paren">(</span><em class="sig-param">sigma=0.4</em>, <em class="sig-param">reset_always=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_transformer.html#GuidedMultiHeadAttentionLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.GuidedMultiHeadAttentionLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss" title="espnet.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.pytorch_backend.e2e_tts_tacotron2.GuidedAttentionLoss</span></code></a></p>
<p>Guided attention loss function module for multi head attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sigma</strong> (<em>float</em><em>, </em><em>optional</em>) – Standard deviation to control how close attention to a diagonal.</p></li>
<li><p><strong>reset_always</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to always reset masks.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.GuidedMultiHeadAttentionLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">att_ws</em>, <em class="sig-param">ilens</em>, <em class="sig-param">olens</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_transformer.html#GuidedMultiHeadAttentionLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.GuidedMultiHeadAttentionLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>att_ws</strong> (<em>Tensor</em>) – Batch of multi head attention weights (B, H, T_max_out, T_max_in).</p></li>
<li><p><strong>ilens</strong> (<em>LongTensor</em>) – Batch of input lenghts (B,).</p></li>
<li><p><strong>olens</strong> (<em>LongTensor</em>) – Batch of output lenghts (B,).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Guided attention loss value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.TTSPlot">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_tts_transformer.</code><code class="sig-name descname">TTSPlot</code><span class="sig-paren">(</span><em class="sig-param">att_vis_fn</em>, <em class="sig-param">data</em>, <em class="sig-param">outdir</em>, <em class="sig-param">converter</em>, <em class="sig-param">transform</em>, <em class="sig-param">device</em>, <em class="sig-param">reverse=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_transformer.html#TTSPlot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.TTSPlot" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.pytorch_backend.transformer.plot.PlotAttentionReport</span></code></p>
<p>Attention plot module for TTS-Transformer.</p>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.TTSPlot.plotfn">
<code class="sig-name descname">plotfn</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">attn_dict</em>, <em class="sig-param">outdir</em>, <em class="sig-param">suffix='png'</em>, <em class="sig-param">savefn=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_transformer.html#TTSPlot.plotfn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.TTSPlot.plotfn" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot multi head attentions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>dict</em>) – Utts info from json file.</p></li>
<li><p><strong>attn_dict</strong> (<em>dict</em>) – Multi head attention dict.
Values should be numpy.ndarray (H, L, T)</p></li>
<li><p><strong>outdir</strong> (<em>str</em>) – Directory name to save figures.</p></li>
<li><p><strong>suffix</strong> (<em>str</em>) – Filename suffix including image type (e.g., png).</p></li>
<li><p><strong>savefn</strong> (<em>function</em>) – Function to save figures.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_tts_transformer.</code><code class="sig-name descname">Transformer</code><span class="sig-paren">(</span><em class="sig-param">idim</em>, <em class="sig-param">odim</em>, <em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_transformer.html#Transformer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#espnet.nets.tts_interface.TTSInterface" title="espnet.nets.tts_interface.TTSInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">espnet.nets.tts_interface.TTSInterface</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Text-to-Speech Transformer module.</p>
<p>This is a module of text-to-speech Transformer described in <a class="reference external" href="https://arxiv.org/pdf/1809.08895.pdf">Neural Speech Synthesis with Transformer Network</a>,
which convert the sequence of characters or phonemes into the sequence of Mel-filterbanks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idim</strong> (<em>int</em>) – Dimension of the inputs.</p></li>
<li><p><strong>odim</strong> (<em>int</em>) – Dimension of the outputs.</p></li>
<li><p><strong>args</strong> (<em>Namespace</em>) – <ul>
<li><p>embed_dim (int): Dimension of character embedding.</p></li>
<li><p>eprenet_conv_layers (int): Number of encoder prenet convolution layers.</p></li>
<li><p>eprenet_conv_chans (int): Number of encoder prenet convolution channels.</p></li>
<li><p>eprenet_conv_filts (int): Filter size of encoder prenet convolution.</p></li>
<li><p>dprenet_layers (int): Number of decoder prenet layers.</p></li>
<li><p>dprenet_units (int): Number of decoder prenet hidden units.</p></li>
<li><p>elayers (int): Number of encoder layers.</p></li>
<li><p>eunits (int): Number of encoder hidden units.</p></li>
<li><p>adim (int): Number of attention transformation dimensions.</p></li>
<li><p>aheads (int): Number of heads for multi head attention.</p></li>
<li><p>dlayers (int): Number of decoder layers.</p></li>
<li><p>dunits (int): Number of decoder hidden units.</p></li>
<li><p>postnet_layers (int): Number of postnet layers.</p></li>
<li><p>postnet_chans (int): Number of postnet channels.</p></li>
<li><p>postnet_filts (int): Filter size of postnet.</p></li>
<li><p>use_scaled_pos_enc (bool): Whether to use trainable scaled positional encoding.</p></li>
<li><p>use_batch_norm (bool): Whether to use batch normalization in encoder prenet.</p></li>
<li><p>encoder_normalize_before (bool): Whether to perform layer normalization before encoder block.</p></li>
<li><p>decoder_normalize_before (bool): Whether to perform layer normalization before decoder block.</p></li>
<li><p>encoder_concat_after (bool): Whether to concatenate attention layer’s input and output in encoder.</p></li>
<li><p>decoder_concat_after (bool): Whether to concatenate attention layer’s input and output in decoder.</p></li>
<li><p>reduction_factor (int): Reduction factor.</p></li>
<li><p>transformer_init (float): How to initialize transformer parameters.</p></li>
<li><p>transformer_lr (float): Initial value of learning rate.</p></li>
<li><p>transformer_warmup_steps (int): Optimizer warmup steps.</p></li>
<li><p>transformer_enc_dropout_rate (float): Dropout rate in encoder except for attention &amp; positional encoding.</p></li>
<li><p>transformer_enc_positional_dropout_rate (float): Dropout rate after encoder positional encoding.</p></li>
<li><p>transformer_enc_attn_dropout_rate (float): Dropout rate in encoder self-attention module.</p></li>
<li><p>transformer_dec_dropout_rate (float): Dropout rate in decoder except for attention &amp; positional encoding.</p></li>
<li><p>transformer_dec_positional_dropout_rate (float): Dropout rate after decoder positional encoding.</p></li>
<li><p>transformer_dec_attn_dropout_rate (float): Dropout rate in deocoder self-attention module.</p></li>
<li><p>transformer_enc_dec_attn_dropout_rate (float): Dropout rate in encoder-deocoder attention module.</p></li>
<li><p>eprenet_dropout_rate (float): Dropout rate in encoder prenet.</p></li>
<li><p>dprenet_dropout_rate (float): Dropout rate in decoder prenet.</p></li>
<li><p>postnet_dropout_rate (float): Dropout rate in postnet.</p></li>
<li><p>use_masking (bool): Whether to use masking in calculation of loss.</p></li>
<li><p>bce_pos_weight (float): Positive sample weight in bce calculation (only for use_masking=true).</p></li>
<li><p>loss_type (str): How to calculate loss.</p></li>
<li><p>use_guided_attn_loss (bool): Whether to use guided attention loss.</p></li>
<li><p>num_heads_applied_guided_attn (int): Number of heads in each layer to be applied guided attention loss.</p></li>
<li><p>num_layers_applied_guided_attn (int): Number of layers to be applied guided attention loss.</p></li>
<li><p>modules_applied_guided_attn (list): List of module names to be applied guided attention loss.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.add_arguments">
<em class="property">static </em><code class="sig-name descname">add_arguments</code><span class="sig-paren">(</span><em class="sig-param">parser</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_transformer.html#Transformer.add_arguments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.add_arguments" title="Permalink to this definition">¶</a></dt>
<dd><p>Add model-specific arguments to the parser.</p>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.attention_plot_class">
<em class="property">property </em><code class="sig-name descname">attention_plot_class</code><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.attention_plot_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Return plot class for attention weight plot.</p>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.base_plot_keys">
<em class="property">property </em><code class="sig-name descname">base_plot_keys</code><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.base_plot_keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Return base key names to plot during training. keys should match what <cite>chainer.reporter</cite> reports.</p>
<p>If you add the key <cite>loss</cite>, the reporter will report <cite>main/loss</cite> and <cite>validation/main/loss</cite> values.
also <cite>loss.png</cite> will be created as a figure visulizing <cite>main/loss</cite> and <cite>validation/main/loss</cite> values.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of strings which are base keys to plot during training.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.calculate_all_attentions">
<code class="sig-name descname">calculate_all_attentions</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em>, <em class="sig-param">olens</em>, <em class="sig-param">skip_output=False</em>, <em class="sig-param">keep_tensor=False</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_transformer.html#Transformer.calculate_all_attentions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate all of the attention weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>Tensor</em>) – Batch of padded character ids (B, Tmax).</p></li>
<li><p><strong>ilens</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</p></li>
<li><p><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</p></li>
<li><p><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</p></li>
<li><p><strong>skip_output</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to skip calculate the final output.</p></li>
<li><p><strong>keep_tensor</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to keep original tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dict of attention weights and outputs.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">ilens</em>, <em class="sig-param">ys</em>, <em class="sig-param">labels</em>, <em class="sig-param">olens</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_transformer.html#Transformer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>Tensor</em>) – Batch of padded character ids (B, Tmax).</p></li>
<li><p><strong>ilens</strong> (<em>LongTensor</em>) – Batch of lengths of each input batch (B,).</p></li>
<li><p><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</p></li>
<li><p><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loss value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.inference">
<code class="sig-name descname">inference</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">inference_args</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_transformer.html#Transformer.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.Transformer.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate the sequence of features given the sequences of characters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – Input sequence of characters (T,).</p></li>
<li><p><strong>inference_args</strong> (<em>Namespace</em>) – <ul>
<li><p>threshold (float): Threshold in inference.</p></li>
<li><p>minlenratio (float): Minimum length ratio in inference.</p></li>
<li><p>maxlenratio (float): Maximum length ratio in inference.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output sequence of features (L, odim).
Tensor: Output sequence of stop probabilities (L,).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.TransformerLoss">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.pytorch_backend.e2e_tts_transformer.</code><code class="sig-name descname">TransformerLoss</code><span class="sig-paren">(</span><em class="sig-param">args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_transformer.html#TransformerLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.TransformerLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Loss function module for TTS-Transformer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> (<em>Namespace</em>) – <ul class="simple">
<li><p>use_masking (bool): Whether to mask padded part in loss calculation.</p></li>
<li><p>bce_pos_weight (float): Weight of positive sample of stop token (only for use_masking=True).</p></li>
</ul>
</p>
</dd>
</dl>
<dl class="method">
<dt id="espnet.nets.pytorch_backend.e2e_tts_transformer.TransformerLoss.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">after_outs</em>, <em class="sig-param">before_outs</em>, <em class="sig-param">logits</em>, <em class="sig-param">ys</em>, <em class="sig-param">labels</em>, <em class="sig-param">olens</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/e2e_tts_transformer.html#TransformerLoss.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.e2e_tts_transformer.TransformerLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate forward propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>after_outs</strong> (<em>Tensor</em>) – Batch of outputs after postnets (B, Lmax, odim).</p></li>
<li><p><strong>before_outs</strong> (<em>Tensor</em>) – Batch of outputs before postnets (B, Lmax, odim).</p></li>
<li><p><strong>logits</strong> (<em>Tensor</em>) – Batch of stop logits (B, Lmax).</p></li>
<li><p><strong>ys</strong> (<em>Tensor</em>) – Batch of padded target features (B, Lmax, odim).</p></li>
<li><p><strong>labels</strong> (<em>LongTensor</em>) – Batch of the sequences of stop token labels (B, Lmax).</p></li>
<li><p><strong>olens</strong> (<em>LongTensor</em>) – Batch of the lengths of each target (B,).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>L1 loss value.
Tensor: Mean square error loss value.
Tensor: Binary cross entropy loss value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="espnet-nets-pytorch-backend-fastspeech">
<span id="id25"></span><h2>espnet.nets.pytorch_backend.fastspeech<a class="headerlink" href="#espnet-nets-pytorch-backend-fastspeech" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.fastspeech"></span></div>
<div class="section" id="espnet-nets-pytorch-backend-frontends">
<span id="id26"></span><h2>espnet.nets.pytorch_backend.frontends<a class="headerlink" href="#espnet-nets-pytorch-backend-frontends" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.frontends"></span></div>
<div class="section" id="espnet-nets-pytorch-backend-nets-utils">
<span id="id27"></span><h2>espnet.nets.pytorch_backend.nets_utils<a class="headerlink" href="#espnet-nets-pytorch-backend-nets-utils" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.nets_utils"></span><p>Network related utility tools.</p>
<dl class="function">
<dt id="espnet.nets.pytorch_backend.nets_utils.make_non_pad_mask">
<code class="sig-prename descclassname">espnet.nets.pytorch_backend.nets_utils.</code><code class="sig-name descname">make_non_pad_mask</code><span class="sig-paren">(</span><em class="sig-param">lengths</em>, <em class="sig-param">xs=None</em>, <em class="sig-param">length_dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/nets_utils.html#make_non_pad_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.nets_utils.make_non_pad_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Make mask tensor containing indices of non-padded part.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lengths</strong> (<em>LongTensor</em><em> or </em><em>List</em>) – Batch of lengths (B,).</p></li>
<li><p><strong>xs</strong> (<em>Tensor</em><em>, </em><em>optional</em>) – The reference tensor. If set, masks will be the same shape as this tensor.</p></li>
<li><p><strong>length_dim</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension indicator of the above tensor. See the example.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>mask tensor containing indices of padded part.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ByteTensor</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>With only lengths.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">make_non_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
<span class="go">masks = [[1, 1, 1, 1 ,1],</span>
<span class="go">         [1, 1, 1, 0, 0],</span>
<span class="go">         [1, 1, 0, 0, 0]]</span>
</pre></div>
</div>
<p>With the reference tensor.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">make_non_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
<span class="go">tensor([[[1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1]],</span>
<span class="go">        [[1, 1, 1, 0],</span>
<span class="go">         [1, 1, 1, 0]],</span>
<span class="go">        [[1, 1, 0, 0],</span>
<span class="go">         [1, 1, 0, 0]]], dtype=torch.uint8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">make_non_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
<span class="go">tensor([[[1, 1, 1, 1, 1, 0],</span>
<span class="go">         [1, 1, 1, 1, 1, 0]],</span>
<span class="go">        [[1, 1, 1, 0, 0, 0],</span>
<span class="go">         [1, 1, 1, 0, 0, 0]],</span>
<span class="go">        [[1, 1, 0, 0, 0, 0],</span>
<span class="go">         [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)</span>
</pre></div>
</div>
<p>With the reference tensor and dimension indicator.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">make_non_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[[1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [0, 0, 0, 0, 0, 0]],</span>
<span class="go">        [[1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0]],</span>
<span class="go">        [[1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">make_non_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[[1, 1, 1, 1, 1, 0],</span>
<span class="go">         [1, 1, 1, 1, 1, 0],</span>
<span class="go">         [1, 1, 1, 1, 1, 0],</span>
<span class="go">         [1, 1, 1, 1, 1, 0],</span>
<span class="go">         [1, 1, 1, 1, 1, 0],</span>
<span class="go">         [1, 1, 1, 1, 1, 0]],</span>
<span class="go">        [[1, 1, 1, 0, 0, 0],</span>
<span class="go">         [1, 1, 1, 0, 0, 0],</span>
<span class="go">         [1, 1, 1, 0, 0, 0],</span>
<span class="go">         [1, 1, 1, 0, 0, 0],</span>
<span class="go">         [1, 1, 1, 0, 0, 0],</span>
<span class="go">         [1, 1, 1, 0, 0, 0]],</span>
<span class="go">        [[1, 1, 0, 0, 0, 0],</span>
<span class="go">         [1, 1, 0, 0, 0, 0],</span>
<span class="go">         [1, 1, 0, 0, 0, 0],</span>
<span class="go">         [1, 1, 0, 0, 0, 0],</span>
<span class="go">         [1, 1, 0, 0, 0, 0],</span>
<span class="go">         [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="espnet.nets.pytorch_backend.nets_utils.make_pad_mask">
<code class="sig-prename descclassname">espnet.nets.pytorch_backend.nets_utils.</code><code class="sig-name descname">make_pad_mask</code><span class="sig-paren">(</span><em class="sig-param">lengths</em>, <em class="sig-param">xs=None</em>, <em class="sig-param">length_dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/nets_utils.html#make_pad_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.nets_utils.make_pad_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Make mask tensor containing indices of padded part.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lengths</strong> (<em>LongTensor</em><em> or </em><em>List</em>) – Batch of lengths (B,).</p></li>
<li><p><strong>xs</strong> (<em>Tensor</em><em>, </em><em>optional</em>) – The reference tensor. If set, masks will be the same shape as this tensor.</p></li>
<li><p><strong>length_dim</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension indicator of the above tensor. See the example.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Mask tensor containing indices of padded part.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>With only lengths.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">make_non_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
<span class="go">masks = [[0, 0, 0, 0 ,0],</span>
<span class="go">         [0, 0, 0, 1, 1],</span>
<span class="go">         [0, 0, 1, 1, 1]]</span>
</pre></div>
</div>
<p>With the reference tensor.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">make_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
<span class="go">tensor([[[0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0]],</span>
<span class="go">        [[0, 0, 0, 1],</span>
<span class="go">         [0, 0, 0, 1]],</span>
<span class="go">        [[0, 0, 1, 1],</span>
<span class="go">         [0, 0, 1, 1]]], dtype=torch.uint8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">make_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
<span class="go">tensor([[[0, 0, 0, 0, 0, 1],</span>
<span class="go">         [0, 0, 0, 0, 0, 1]],</span>
<span class="go">        [[0, 0, 0, 1, 1, 1],</span>
<span class="go">         [0, 0, 0, 1, 1, 1]],</span>
<span class="go">        [[0, 0, 1, 1, 1, 1],</span>
<span class="go">         [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)</span>
</pre></div>
</div>
<p>With the reference tensor and dimension indicator.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">make_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[[0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [1, 1, 1, 1, 1, 1]],</span>
<span class="go">        [[0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1]],</span>
<span class="go">        [[0, 0, 0, 0, 0, 0],</span>
<span class="go">         [0, 0, 0, 0, 0, 0],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1],</span>
<span class="go">         [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">make_pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[[0, 0, 0, 0, 0, 1],</span>
<span class="go">         [0, 0, 0, 0, 0, 1],</span>
<span class="go">         [0, 0, 0, 0, 0, 1],</span>
<span class="go">         [0, 0, 0, 0, 0, 1],</span>
<span class="go">         [0, 0, 0, 0, 0, 1],</span>
<span class="go">         [0, 0, 0, 0, 0, 1]],</span>
<span class="go">        [[0, 0, 0, 1, 1, 1],</span>
<span class="go">         [0, 0, 0, 1, 1, 1],</span>
<span class="go">         [0, 0, 0, 1, 1, 1],</span>
<span class="go">         [0, 0, 0, 1, 1, 1],</span>
<span class="go">         [0, 0, 0, 1, 1, 1],</span>
<span class="go">         [0, 0, 0, 1, 1, 1]],</span>
<span class="go">        [[0, 0, 1, 1, 1, 1],</span>
<span class="go">         [0, 0, 1, 1, 1, 1],</span>
<span class="go">         [0, 0, 1, 1, 1, 1],</span>
<span class="go">         [0, 0, 1, 1, 1, 1],</span>
<span class="go">         [0, 0, 1, 1, 1, 1],</span>
<span class="go">         [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="espnet.nets.pytorch_backend.nets_utils.mask_by_length">
<code class="sig-prename descclassname">espnet.nets.pytorch_backend.nets_utils.</code><code class="sig-name descname">mask_by_length</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">lengths</em>, <em class="sig-param">fill=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/nets_utils.html#mask_by_length"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.nets_utils.mask_by_length" title="Permalink to this definition">¶</a></dt>
<dd><p>Mask tensor according to length.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>Tensor</em>) – Batch of input tensor (B, <cite>*</cite>).</p></li>
<li><p><strong>lengths</strong> (<em>LongTensor</em><em> or </em><em>List</em>) – Batch of lengths (B,).</p></li>
<li><p><strong>fill</strong> (<em>int</em><em> or </em><em>float</em>) – Value to fill masked part.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Batch of masked input tensor (B, <cite>*</cite>).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[1, 2, 3, 4, 5],</span>
<span class="go">        [1, 2, 3, 4, 5],</span>
<span class="go">        [1, 2, 3, 4, 5]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask_by_length</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
<span class="go">tensor([[1, 2, 3, 4, 5],</span>
<span class="go">        [1, 2, 3, 0, 0],</span>
<span class="go">        [1, 2, 0, 0, 0]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="espnet.nets.pytorch_backend.nets_utils.pad_list">
<code class="sig-prename descclassname">espnet.nets.pytorch_backend.nets_utils.</code><code class="sig-name descname">pad_list</code><span class="sig-paren">(</span><em class="sig-param">xs</em>, <em class="sig-param">pad_value</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/nets_utils.html#pad_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.nets_utils.pad_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform padding for the list of tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xs</strong> (<em>List</em>) – List of Tensors [(T_1, <cite>*</cite>), (T_2, <cite>*</cite>), …, (T_B, <cite>*</cite>)].</p></li>
<li><p><strong>pad_value</strong> (<em>float</em>) – Value for padding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Padded tensor (B, Tmax, <cite>*</cite>).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">[tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pad_list</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([[1., 1., 1., 1.],</span>
<span class="go">        [1., 1., 0., 0.],</span>
<span class="go">        [1., 0., 0., 0.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="espnet.nets.pytorch_backend.nets_utils.th_accuracy">
<code class="sig-prename descclassname">espnet.nets.pytorch_backend.nets_utils.</code><code class="sig-name descname">th_accuracy</code><span class="sig-paren">(</span><em class="sig-param">pad_outputs</em>, <em class="sig-param">pad_targets</em>, <em class="sig-param">ignore_label</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/nets_utils.html#th_accuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.nets_utils.th_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pad_outputs</strong> (<em>Tensor</em>) – Prediction tensors (B * Lmax, D).</p></li>
<li><p><strong>pad_targets</strong> (<em>LongTensor</em>) – Target label tensors (B, Lmax, D).</p></li>
<li><p><strong>ignore_label</strong> (<em>int</em>) – Ignore label id.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Accuracy value (0.0 - 1.0).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet.nets.pytorch_backend.nets_utils.to_device">
<code class="sig-prename descclassname">espnet.nets.pytorch_backend.nets_utils.</code><code class="sig-name descname">to_device</code><span class="sig-paren">(</span><em class="sig-param">m</em>, <em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/nets_utils.html#to_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.nets_utils.to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Send tensor into the device of the module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>m</strong> (<em>torch.nn.Module</em>) – Torch module.</p></li>
<li><p><strong>x</strong> (<em>Tensor</em>) – Torch tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Torch tensor located in the same place as torch module.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="espnet.nets.pytorch_backend.nets_utils.to_torch_tensor">
<code class="sig-prename descclassname">espnet.nets.pytorch_backend.nets_utils.</code><code class="sig-name descname">to_torch_tensor</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/pytorch_backend/nets_utils.html#to_torch_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.pytorch_backend.nets_utils.to_torch_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Change to torch.Tensor or ComplexTensor from numpy.ndarray.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – Inputs. It should be one of numpy.ndarray, Tensor, ComplexTensor, and dict.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Type converted inputs.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor or ComplexTensor</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="n">to_torch_tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="go">tensor([1., 1., 1.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">to_torch_tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="ow">is</span> <span class="n">xs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;real&#39;</span><span class="p">:</span> <span class="n">xs</span><span class="p">,</span> <span class="s1">&#39;imag&#39;</span><span class="p">:</span> <span class="n">xs</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">to_torch_tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="go">ComplexTensor(</span>
<span class="go">Real:</span>
<span class="go">tensor([1., 1., 1.])</span>
<span class="go">Imag;</span>
<span class="go">tensor([1., 1., 1.])</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="espnet-nets-pytorch-backend-rnn">
<span id="id28"></span><h2>espnet.nets.pytorch_backend.rnn<a class="headerlink" href="#espnet-nets-pytorch-backend-rnn" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.rnn"></span></div>
<div class="section" id="espnet-nets-pytorch-backend-streaming">
<span id="id29"></span><h2>espnet.nets.pytorch_backend.streaming<a class="headerlink" href="#espnet-nets-pytorch-backend-streaming" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.streaming"></span></div>
<div class="section" id="espnet-nets-pytorch-backend-tacotron2">
<span id="id30"></span><h2>espnet.nets.pytorch_backend.tacotron2<a class="headerlink" href="#espnet-nets-pytorch-backend-tacotron2" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.tacotron2"></span></div>
<div class="section" id="espnet-nets-pytorch-backend-transformer">
<span id="id31"></span><h2>espnet.nets.pytorch_backend.transformer<a class="headerlink" href="#espnet-nets-pytorch-backend-transformer" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.pytorch_backend.transformer"></span></div>
<div class="section" id="espnet-nets-tts-interface">
<span id="id32"></span><h2>espnet.nets.tts_interface<a class="headerlink" href="#espnet-nets-tts-interface" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-espnet.nets.tts_interface"></span><dl class="class">
<dt id="espnet.nets.tts_interface.Reporter">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.tts_interface.</code><code class="sig-name descname">Reporter</code><span class="sig-paren">(</span><em class="sig-param">**links</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/tts_interface.html#Reporter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.tts_interface.Reporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">chainer.link.Chain</span></code></p>
<dl class="method">
<dt id="espnet.nets.tts_interface.Reporter.report">
<code class="sig-name descname">report</code><span class="sig-paren">(</span><em class="sig-param">dicts</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/tts_interface.html#Reporter.report"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.tts_interface.Reporter.report" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="espnet.nets.tts_interface.TTSInterface">
<em class="property">class </em><code class="sig-prename descclassname">espnet.nets.tts_interface.</code><code class="sig-name descname">TTSInterface</code><a class="reference internal" href="../_modules/espnet/nets/tts_interface.html#TTSInterface"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.tts_interface.TTSInterface" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>TTS Interface for ESPnet model implementation</p>
<dl class="method">
<dt id="espnet.nets.tts_interface.TTSInterface.add_arguments">
<em class="property">static </em><code class="sig-name descname">add_arguments</code><span class="sig-paren">(</span><em class="sig-param">parser</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/tts_interface.html#TTSInterface.add_arguments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.tts_interface.TTSInterface.add_arguments" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.tts_interface.TTSInterface.attention_plot_class">
<em class="property">property </em><code class="sig-name descname">attention_plot_class</code><a class="headerlink" href="#espnet.nets.tts_interface.TTSInterface.attention_plot_class" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="espnet.nets.tts_interface.TTSInterface.base_plot_keys">
<em class="property">property </em><code class="sig-name descname">base_plot_keys</code><a class="headerlink" href="#espnet.nets.tts_interface.TTSInterface.base_plot_keys" title="Permalink to this definition">¶</a></dt>
<dd><p>base key names to plot during training. keys should match what <cite>chainer.reporter</cite> reports</p>
<p>if you add the key <cite>loss</cite>, the reporter will report <cite>main/loss</cite> and <cite>validation/main/loss</cite> values.
also <cite>loss.png</cite> will be created as a figure visulizing <cite>main/loss</cite> and <cite>validation/main/loss</cite> values.</p>
<dl class="field-list simple">
<dt class="field-odd">Rtype list[str] plot_keys</dt>
<dd class="field-odd"><p>base keys to plot during training</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.tts_interface.TTSInterface.calculate_all_attentions">
<code class="sig-name descname">calculate_all_attentions</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/tts_interface.html#TTSInterface.calculate_all_attentions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.tts_interface.TTSInterface.calculate_all_attentions" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate TTS attention weights</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>attention weights (B, Lmax, Tmax)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>numpy array</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.tts_interface.TTSInterface.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/tts_interface.html#TTSInterface.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.tts_interface.TTSInterface.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate TTS forward propagation</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>loss value</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="espnet.nets.tts_interface.TTSInterface.inference">
<code class="sig-name descname">inference</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/espnet/nets/tts_interface.html#TTSInterface.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#espnet.nets.tts_interface.TTSInterface.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates the sequence of features given the sequences of characters</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the sequence of features (L, odim)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>torch.Tensor</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the sequence of stop probabilities (L)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>torch.Tensor</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the sequence of attention weight (L, T)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="espnet-transform.html" class="btn btn-neutral float-right" title="espnet.transform package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="espnet-lm.html" class="btn btn-neutral float-left" title="espnet.lm package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Shinji Watanabe

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>